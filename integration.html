<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quiz App - Integration Architect</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        #gotoInput {
            font-size: 1rem;
            transition: 0.3s;
        }

        #gotoInput:focus {
            outline: none;
            border-color: #4CAF50;
            box-shadow: 0 0 5px rgba(76, 175, 80, 0.5);
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
            animation: slideIn 0.6s ease-out;
        }
        #explanationBox {
            margin-top: 20px;
            background: #e3f2fd;
            color: #0d47a1;
            padding: 15px;
            border-left: 4px solid #2196F3;
            border-radius: 8px;
        }


        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .header {
            background: linear-gradient(135deg, #4CAF50, #45a049);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .progress-container {
            background: rgba(255,255,255,0.2);
            border-radius: 10px;
            padding: 5px;
            margin: 20px 0;
        }

        .progress-bar {
            background: white;
            height: 10px;
            border-radius: 5px;
            transition: width 0.3s ease;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
        }

        .stats {
            display: flex;
            justify-content: space-around;
            margin-top: 20px;
        }

        .stat {
            text-align: center;
        }

        .stat-number {
            font-size: 1.5rem;
            font-weight: bold;
            display: block;
        }

        .question-container {
            padding: 40px;
            min-height: 400px;
        }

        .question {
            margin-bottom: 30px;
        }

        .question-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }

        .question-number {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 10px 20px;
            border-radius: 25px;
            font-weight: bold;
            font-size: 1.1rem;
        }

        .question-type {
            background: #f0f8ff;
            color: #2196F3;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.9rem;
            font-weight: 500;
            border: 2px solid #e3f2fd;
        }

        .question-text {
            font-size: 1.2rem;
            line-height: 1.6;
            margin-bottom: 25px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 4px solid #4CAF50;
        }

        .options {
            display: grid;
            gap: 15px;
        }

        .option {
            position: relative;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .option input {
            position: absolute;
            opacity: 0;
            cursor: pointer;
        }

        .option-label {
            display: flex;
            align-items: center;
            padding: 15px 20px;
            background: #f8f9fa;
            border: 2px solid #e9ecef;
            border-radius: 10px;
            font-size: 1rem;
            line-height: 1.5;
            transition: all 0.3s ease;
        }

        .option:hover .option-label {
            background: #e3f2fd;
            border-color: #2196F3;
            transform: translateX(5px);
        }

        .option input:checked + .option-label {
            background: linear-gradient(135deg, #4CAF50, #45a049);
            color: white;
            border-color: #4CAF50;
            transform: translateX(5px);
        }

        .checkmark {
            width: 20px;
            height: 20px;
            border: 2px solid #ddd;
            border-radius: 4px;
            margin-right: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }

        .option input:checked + .option-label .checkmark {
            background: white;
            border-color: white;
        }

        .option input:checked + .option-label .checkmark::after {
            content: '✓';
            color: #4CAF50;
            font-weight: bold;
        }

        .navigation {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: #f8f9fa;
            border-top: 1px solid #e9ecef;
        }

        .btn {
            padding: 12px 25px;
            border: none;
            border-radius: 25px;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .btn-primary {
            background: linear-gradient(135deg, #4CAF50, #45a049);
            color: white;
        }

        .btn-secondary {
            background: #6c757d;
            color: white;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }

        .btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .controls {
            padding: 20px 40px;
            background: #f8f9fa;
            border-top: 1px solid #e9ecef;
        }

        .randomize-controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            align-items: center;
        }

        .btn-toggle {
            background: #e9ecef;
            color: #495057;
            transition: all 0.3s ease;
        }

        .btn-toggle.active {
            background: linear-gradient(135deg, #FF6B6B, #FF8E8E);
            color: white;
        }

        .btn-toggle:hover {
            background: #dee2e6;
        }

        .btn-toggle.active:hover {
            background: linear-gradient(135deg, #FF5252, #FF7979);
        }

        .results {
            text-align: center;
            padding: 40px;
        }

        .score {
            font-size: 4rem;
            font-weight: bold;
            color: #4CAF50;
            margin: 20px 0;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }

        .score.low {
            color: #f44336;
        }

        .score.medium {
            color: #ff9800;
        }

        .results-details {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .result-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #4CAF50;
        }

        .result-card.incorrect {
            border-left-color: #f44336;
        }

        .result-title {
            font-weight: bold;
            font-size: 1.1rem;
            margin-bottom: 10px;
        }

        .hidden {
            display: none;
        }

        .fade-in {
            animation: fadeIn 0.5s ease-in;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .celebration {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            z-index: 1000;
        }

        .option-label.correct {
            background: #d4edda !important;
            border-color: #28a745 !important;
            color: #155724 !important;
        }

        .option-label.incorrect {
            background: #f8d7da !important;
            border-color: #dc3545 !important;
            color: #721c24 !important;
        }

        @media (max-width: 768px) {
            body {
                padding: 8px;
                background: #667eea;
            }
            
            .container {
                border-radius: 12px;
                box-shadow: 0 8px 20px rgba(0,0,0,0.15);
            }
            
            .header {
                padding: 15px;
            }
            
            .header h1 {
                font-size: 1.6rem;
                margin-bottom: 8px;
            }
            
            .progress-container {
                margin: 12px 0;
                padding: 3px;
            }
            
            .progress-bar {
                height: 8px;
            }
            
            .stats {
                margin-top: 12px;
                gap: 8px;
            }
            
            .stat {
                flex: 1;
            }
            
            .stat-number {
                font-size: 1.2rem;
            }
            
            .stat span:last-child {
                font-size: 0.85rem;
            }
            
            .question-container {
                padding: 12px;
                min-height: auto;
            }
            
            .question {
                margin-bottom: 12px;
            }
            
            .question-header {
                flex-direction: column;
                gap: 8px;
                margin-bottom: 12px;
                align-items: stretch;
            }
            
            .question-number {
                padding: 8px 12px;
                font-size: 0.95rem;
                text-align: center;
                border-radius: 8px;
            }
            
            .question-type {
                padding: 6px 12px;
                font-size: 0.8rem;
                text-align: center;
                border-radius: 8px;
                border-width: 1px;
            }
            
            .question-text {
                font-size: 1rem;
                line-height: 1.4;
                margin-bottom: 15px;
                padding: 12px;
                border-radius: 8px;
                border-left-width: 3px;
            }
            
            .options {
                gap: 8px;
            }
            
            .option-label {
                padding: 10px 12px;
                font-size: 0.9rem;
                line-height: 1.3;
                border-radius: 8px;
                border-width: 1px;
            }
            
            .option:hover .option-label {
                transform: none;
            }
            
            .option input:checked + .option-label {
                transform: none;
            }
            
            .checkmark {
                width: 16px;
                height: 16px;
                margin-right: 10px;
                flex-shrink: 0;
            }
            
            .navigation {
                padding: 12px;
                flex-direction: row;
                gap: 8px;
            }
            
            .btn {
                padding: 10px 16px;
                font-size: 0.85rem;
                border-radius: 8px;
                letter-spacing: 0.3px;
                flex: 1;
            }
            
            .controls {
                padding: 12px;
            }
            
            .randomize-controls {
                flex-direction: column;
                gap: 8px;
            }
            
            #gotoInput {
                width: 100%;
                max-width: 120px;
                padding: 8px;
                font-size: 0.9rem;
                border-radius: 6px;
            }
            
            .btn-toggle {
                width: 100%;
                padding: 8px 12px;
                font-size: 0.8rem;
                border-radius: 6px;
            }
            
            #explanationBox {
                margin-top: 12px;
                padding: 10px;
                font-size: 0.9rem;
                line-height: 1.3;
                border-left-width: 3px;
                border-radius: 6px;
            }
            
            .results {
                padding: 20px 12px;
            }
            
            .score {
                font-size: 2.5rem;
                margin: 15px 0;
            }
            
            .results-details {
                grid-template-columns: 1fr 1fr;
                gap: 12px;
                margin: 20px 0;
            }
            
            .result-card {
                padding: 12px;
                border-radius: 8px;
                border-left-width: 3px;
            }
            
            .result-title {
                font-size: 0.95rem;
                margin-bottom: 6px;
            }
            
            /* Ottimizzazioni specifiche per schermi molto piccoli */
            @media (max-width: 480px) {
                .results-details {
                    grid-template-columns: 1fr;
                }
                
                .question-text {
                    font-size: 0.95rem;
                    padding: 10px;
                }
                
                .option-label {
                    font-size: 0.85rem;
                    padding: 8px 10px;
                }
                
                .header h1 {
                    font-size: 1.4rem;
                }
                
                .btn {
                    font-size: 0.8rem;
                    padding: 8px 12px;
                }
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><i class="fas fa-graduation-cap"></i> Quiz Integration Architect</h1>
            <div class="progress-container">
                <div class="progress-bar" id="progressBar"></div>
            </div>
            <div class="stats">
                <div class="stat">
                    <span class="stat-number" id="currentQuestion">1</span>
                    <span>Domanda</span>
                </div>
                <div class="stat">
                    <span class="stat-number" id="totalQuestions">0</span>
                    <span>Totale</span>
                </div>
                <div class="stat">
                    <span class="stat-number" id="correctAnswers">0</span>
                    <span>Corrette</span>
                </div>
            </div>
        </div>
        <div class="navigation">
            <button class="btn btn-secondary" id="prevBtn" onclick="previousQuestion()" disabled>
                <i class="fas fa-arrow-left"></i> Precedente
            </button>
            <button class="btn btn-primary" id="nextBtn" onclick="nextQuestion()">
                Prossima <i class="fas fa-arrow-right"></i>
            </button>
        </div>

        <div id="quizContainer" class="question-container">
            <div class="question">
                <div class="question-header">
                    <div class="question-number">
                        Domanda <span id="questionNumber">1</span>
                    </div>
                    <div class="question-type" id="questionType">
                        Risposta Singola
                    </div>
                </div>
                <div class="question-text" id="questionText">
                    Caricamento domanda...
                </div>
                <div class="options" id="optionsContainer">
                    <!-- Le opzioni verranno generate dinamicamente -->
                </div>
                <div style="margin-top: 20px;">
                    <button class="btn btn-toggle" onclick="showCorrectAnswer()">Mostra risposta corretta</button>
                </div>
                <div id="explanationBox" class="question-text hidden" style="border-left-color: #2196F3;">
                    <!-- Spiegazione inserita dinamicamente -->
                </div>

            </div>
        </div>

        <div id="resultsContainer" class="results hidden">
            <i class="fas fa-trophy" style="font-size: 4rem; color: #4CAF50; margin-bottom: 20px;"></i>
            <h2>Quiz Completato!</h2>
            <div class="score" id="finalScore">0%</div>
            <div class="results-details" id="resultsDetails">
                <!-- I dettagli dei risultati verranno generati dinamicamente -->
            </div>
            <button class="btn btn-primary" onclick="restartQuiz()">
                <i class="fas fa-redo"></i> Ricomincia Quiz
            </button>
        </div>
        <div class="navigation">
            <button class="btn btn-secondary" id="prevBtn" onclick="previousQuestion()" disabled>
                <i class="fas fa-arrow-left"></i> Precedente
            </button>
            <button class="btn btn-primary" id="nextBtn" onclick="nextQuestion()">
                Prossima <i class="fas fa-arrow-right"></i>
            </button>
        </div>
        <div class="controls">
            <div class="randomize-controls">
                <input type="number" id="gotoInput" placeholder="N. domanda" min="1" style="padding: 10px; width: 120px; border-radius: 8px; border: 1px solid #ccc;">
                <button class="btn btn-toggle" id="gotobutton" onclick="goToQuestion()">
                    <i class="fas fa-shuffle"></i> Vai alla domanda
                </button>
                <button class="btn btn-toggle" id="randomQuestionsBtn" onclick="toggleRandomQuestions()">
                    <i class="fas fa-random"></i> Domande Random
                </button>
                <button class="btn btn-toggle" id="randomAnswersBtn" onclick="toggleRandomAnswers()">
                    <i class="fas fa-shuffle"></i> Risposte Random
                </button>
            </div>
        </div>

        
    </div>

    <script>
        // Dati delle domande
        const originalQuestions = [{"id":1,"text":"Northern Trail Outfitters submits orders to the manufacturing system web-service. Recently, the system has experienced outages that keeping service unavailable for several days. What solution should an architect recommend to handle errors during these types of service outages?","options":["Use middleware queuing and buffering toinsulate Salesforce from system outages.","A Use Platform Event replayldand custom scheduled Apex process to retrieve missed events.","Use @future jobld and custom scheduled apex process to retry failed service calls.","Use Outbound Messaging to automatically retry failed service calls."],"correct":[0],"explanation":"Using middleware queuing and buffering is a solution that can handle errors during service outages by storing the messages in a queue until the service is available again. This way, Salesforce does not loseany data or encounter any failures when the service is down. Using @future jobld and custom scheduled apex process to retry failed service calls is not a good solution because it can consume a lot of governor limits and create a lot of duplicate records. Using Outbound Messaging to automatically retry failed service calls is also not a good solution because it has a limited number of retries and a fixed retry interval, which may not be sufficient for long service outages. Using Platform Event replayld and custom scheduled Apex process to retrieve missed events is not applicable for this scenario because Platform Events are used for event-driven integration, not for web-service integration. Reference: Salesforce Integration Architecture Designer Resource Guide, page 29- 30"},{"id":2,"text":"Service Agents at Northern Trail Outfitters uses Salesforce to manage cases and B2C Commerce for ordering. Which integration solution shouldan architect recommend in order for the service agents to see order history from a B2C Commerce system?","options":["Salesforce B2C Commerce to Service Cloud Connector","A REST API offered by Commerce Platform","Mulesoft Anypoint Platform","REST API offered by Salesforce Platform"],"correct":[0],"explanation":"Option A is correct because Salesforce B2C Commerce to Service Cloud Connector is an integration solution that allows service agents to see order history from a B2C Commerce system. Salesforce B2C Commerce to Service CloudConnector is a pre-built package that integrates Salesforce B2C Commerce and Service Cloud using REST APIs and Platform Events. It enables service agents to view, edit, cancel, and refund orders from B2C Commerce within the Service Cloud console. It also supports features such as customer verification, order search, order details, and order history. Option B is incorrect because a REST API offered by Commerce Platform is not an integration solution that allows service agents to see order history from a B2CCommerce system. A REST API offered by Commerce Platform is a set of web services that expose the functionality and data of the Commerce Platform to external applications. It can be used to create, update, delete, or query resources such as products, catalogs, customers, or orders. However, a REST API offered by Commerce Platform is not a complete integration solution, as it requires additional development, configuration, and maintenance to connect with Service Cloud and display the order history in the Service Cloud console. Option C is incorrect because Mulesoft Anypoint Platform is not an integration solution that allows service agents to see order history from a B2C Commerce system. Mulesoft Anypoint Platform is a platform that enables developers to build, manage, and monitor integrations and APIs across various systems and applications. It can be used to connect Salesforce B2C Commerce and Service Cloud using various connectors, protocols, and transformations. However, Mulesoft Anypoint Platform is not apre- built integration solution, as it requires additional development, configuration, and maintenance to connect with Service Cloud and display the order history in the Service Cloud console. Option D is incorrect because a REST API offered by SalesforcePlatform is not an integration solution that allows service agents to see order history from a B2C Commerce system. A REST API offered by Salesforce Platform is a set of web services that expose the functionality and data of the Salesforce Platform to external applications. It can be used to create, update, delete, or query resources such as objects, records, metadata, or Apex classes. However, a REST API offered by Salesforce Platform is not a complete integration solution, as it requires additional development, configuration, and maintenance to connect with B2C Commerce and display the order history in the Service Cloud console. References: Salesforce B2C Commerce to Service Cloud Connector : Salesforce B2C Commerce to Service Cloud Connector Implementation Guide : Commerce API Explorer : MuleSoft | Integration Platform for Connecting SaaS and Enterprise Applications : REST API Developer Guide"},{"id":3,"text":"A health care services company maintains a Patient Prescriptions System that has 50+ million records in a secure database. Their customer base and data set growing rapidly. They want to make sure that the following policies are enforced: 1. Identifiable patient prescriptions must exist only in their secure system's database and encrypted at rest. 2. Identifiable patient prescriptions must be made available only to people explicit authorized in the Patient Prescriptions System assigned nurses and doctors, patient, and people explicitly the patient may authorize. 3. Must be availableonly to verified and pre-approved people or legal entities. To enable this, the company provides the following capabilities: 1. One-time use identity tokens for patients, nurses, doctors, and other people that expire within a few minutes. 2. Certificatesfor legal entities. . RESTful services. The company has a Salesforce Community Cloud portal for patients, nurses, doctors, and other authorized people. A limited number of employees analyze deidentified data in Einstein Analytics. Which two capabilities should the integration architect require for the Community Cloud portal and Einstein Analytics? Choose 2 answers","options":["Identity token data storage","Bulk load for Einstein Analytics","Callouts to RESTful services","Encryption in transit and at rest"],"correct":[1,2],"explanation":"The integration architect should require bulk load for Einstein Analytics and callouts to RESTful services for the Community Cloud portal and Einstein Analytics. Bulk load is a capability that allows loading large volumes of data into Einstein Analytics from external sources, such as the Patient Prescriptions System. This can be done securely by using encryption in transit and at rest, and by masking or removing any identifiable patient information from the data. Callouts to RESTful services are a capability that allows making requests to external web services from Salesforce, such as the Patient Prescriptions System. This can be done securely by using identity tokens or certificates for authentication and authorization, and by using encryption in transit and at rest12 References: How to Load Data into Einstein Analytics, Strengthen Your Data’s Security with Shield Platform Encryption"},{"id":4,"text":"An integration architect needs to build a solution that will be using the Streaming API, but the data loss should be minimized, even when the client re-connects every couple of days. Which two types of Streaming API events should be considered? Choose 2 answers","options":["Generic Events","Change Data Capture Events","PushTopic Events","High Volume Platform Events"],"correct":[1,3],"explanation":"The integration architect should consider Change Data Capture Events and High Volume Platform Events for using the Streaming API with minimal data loss. Change Data Capture Events capture changes to Salesforce records and deliver them as events to subscribers. High Volume Platform Events are custom events that can be published and consumed at a large scale. Both types of events support reliable event delivery, which means that events are stored for 72 hours and can be replayed by subscribers in case of connection loss or failure. This ensures that data loss is minimized, even when the client re-connects every couple of days12 References: Change Data Capture Developer Guide, Platform Events DeveloperGuide"},{"id":5,"text":"A call center manager uses a custom dashboard to track Case related metrics. The manager wants a component to display the number of closed Cases in real time. What type of event should be used to meet this requirement?","options":["Push Topic Event","Change Data Capture Event","Platform Event","Generic Event"],"correct":[1],"explanation":"Change Data Capture Event is the best option to meet this requirement. Change Data Capture Event is a type of streamingevent that notifies subscribers of changes to Salesforce records, such as creation, update, delete, and undelete operations1. By subscribing to Change Data Capture Event for the Case object, the dashboard component can receive real-time updates on the number of closed Cases. Push Topic Event is another type of streaming event that notifies subscribers of changes to Salesforce records that match a SOQL query2. However, Push Topic Event has some limitations, such as not supporting all SOQL features and not capturing delete and undelete operations3. Platform Event is a type of streaming event that delivers custom notifications within the Salesforce platform or from external sources4. Platform Event is not suitable for this requirement because it is not tied to Salesforce records and requires custom logic to publish and subscribe. Generic Event is a type of streaming event that sends custom JSON notifications to subscribers without a predefined schema5. Generic Event is not suitable for this requirement because it is not tied to Salesforce records and requires custom logic to publish and subscribe."},{"id":6,"text":"A customer's enterprise architect has identified requirements around caching, queuing, error handling, alerts, retries, event handling, etc. The company has asked the Salesforce integration architect to help fulfill such aspects with their Salesforce program. Which three recommendations should the Salesforce integration architect make? Choose 3 answers","options":["Transform a fire-and-forget mechanism to request-reply should be handled by middleware tools (like ETL/ESB) to improve performance.","Provide true message queueing for integration scenarios (including orchestration, process choreography, quality of service, etc.) given that a middleware solution is required.","Message transformation and protocol translation should be done within Salesforce.Recommend leveraging Salesforce native protocol conversion capabilities as middleware tools are NOT suited for such tasks","Event handling processes such as writing to a log, sending an error or recovery process, or sending an extra message, can be assumed to be handled by middleware.","Event handling in a publish/subscribe scenario, the middleware can be used to route requests or messages to active data-event subscribers from active data-event publishers."],"correct":[1,3,4],"explanation":"The recommendations that the Salesforce integration architect should make to fulfill the requirements around caching, queuing, error handling, alerts, retries, event handling, etc. arebased on the best practices and capabilities of the middleware tools. The recommendation B is valid because middleware tools can provide true message queueing for integration scenarios that require asynchronous processing, guaranteed delivery, load balancing, etc. The recommendation D is valid because middleware tools can handle various events that may occur during the integration process, such as logging errors, sending notifications, triggering recovery actions, etc. The recommendation E is valid becausemiddleware tools can support the publish/subscribe pattern for event-driven integration, where the middleware can route messages from publishers to subscribers based on topics or filters. The recommendation A is not valid because transforming a fire-and-forget mechanism to request-reply does not improve performance, but rather increases the latency and complexity of the integration. The recommendation C is not valid because message transformation and protocol translation are tasks that are better suited for middleware tools than Salesforce, as they can handle different formats and protocols more efficiently and flexibly23 1: Integration Architecture Designer Resource Guide 2: Integration Patterns and Practices 3: Salesforce Integration Architecture Designer Certification Exam Guide"},{"id":7,"text":"A large enterprise customer operating in a high regulated industry is planning to implement Salesforce for customer facing associates in both Sales and Service, and back-office staff. The business processes that Salesforce supports are critical to the business. Salesforce will be integrated to multiple back-office systems to provide a single interface for associates. Reliability and monitoring of these integrations are required as associates support customers. Which integration solution should the architect consider when planning the implementation?","options":["Architect Services in back-office systems to support callouts from Salesforce and build reliability, monitoring and reporting capabilities.","Decouple back-office system callouts into separate distinct services that have inbuilt error logging and monitoring frameworks.","Build a custom integration gateway to support back-office system integrations and ensure reliability and monitoring capabilities.","Leverage Middleware for all back-office system integrations ensuring real time alerting, monitoring and reporting capabilities."],"correct":[3],"explanation":"Leveraging Middleware for all back-office system integrations ensuring real time alerting, monitoring and reporting capabilities is the best integration solution for this scenario.Middleware is a software layer that acts as an intermediary between Salesforce and other systems, providing a common platform for data transformation, routing, orchestration, security, error handling, logging, and monitoring. Middleware can help to ensurereliability and performance of the integrations, as well as provide visibility and control over the integration processes. Middleware can also support various integration patterns and protocols, such as SOAP, REST, JMS, etc. Reference: Salesforce Integration Architecture Designer Resource Guide, page 14"},{"id":8,"text":"The URL for an external service has been changed without prior notice. The service provides up-to-date money exchange rates that are accessedseveral times from Salesforce and are a Dusiness-critical function for end users. Which solutions should an integration architect recommend be implemented to minimize potential downtime for users in this situation?","options":["Named Credentials and Content Security Policies","Remote Site Settings and Named Credentials","Enterprise Service Blus (ESB) and Remote Site Settings"],"correct":[1],"explanation":"Remote Site Settings and Named Credentials are solutions that should be implemented to minimize potential downtime for users in this situation. Remote Site Settings allow you to specify external domains that your organization can access during API calls or integrations. Named Credentials allow you to store authentication information for external services in a secure way. By using Named Credentials, you can easily update the URL of the external service without changing any code or configuration that references it. Enterprise Service Bus (ESB) and Remote Site Settings are not solutions that should be implemented in this situation because ESB is a middleware that facilitates communication between different systems, not a way to update the URL of an external service. Event Monitoring and Content Security Policies are also not solutions that should be implemented in this situation because Event Monitoring is used to track user activity and performance metrics, not to monitor external service availability. Content Security Policies are used to control what resources can be loaded on a web page, not to update the URL of an external service."},{"id":9,"text":"Northern Trail Outfitters (NTO) has an integration set up between a Salesforce org and a quoting system. NTO would like to show a notification to allsales representatives that use Salesforceanytime the quoting system will be taken down for maintenance. Which Salesforce API should an Integration Architect use to fulfill this requirement?","options":["Connect REST API","Tooling API","REST API","Streaming API"],"correct":[3],"explanation":"Streaming API is the best option for sending real-time notifications of changes to Salesforce data. Streaming API uses a publish-subscribe model to push relevant data to subscribers without polling. Streaming API supports PushTopic events, whichare based on SOQL queries that define the data changes to listen for. The affiliate company can subscribe to a PushTopic event on the NTO Salesforce org and receive notifications whenever the data that matches the query changes. This way, the affiliate company can be informed of any updates to the opportunities in the NTO Salesforce instance."},{"id":10,"text":"An Integration Architect has designed a mobile application for Salesforce users to get data while on the road using a custom UI. The application is securedwith oAuth and is currently functioning well. There is a new requirement where the mobile application needs to obtain the GPS coordinates and storeit on a custom geolocation field. The geolocation field is secured with Field Level Security, so users can view the value without changing it. What should be done to meet the requirement?","options":["The mobile device makes a SOAP API inbound call.The mobile device receives a REST Apex callout call.","The mobile device makes a REST API inbound call.","The mobile devicemakes a REST Apex inbound call."],"correct":[2],"explanation":"The mobile device should make a REST Apex inbound call to meet the requirement. A REST Apex inbound call allows the mobile device to invoke custom Apex logic on Salesforce using a RESTful interface. The Apex logiccan then access the geolocation field using the user’s OAuth token and update it with the GPS coordinates from the mobile device. The geolocation field is secured with Field Level Security, so the user can view the value without changing it. A SOAP API inbound call or a REST API inbound call would not be able to access the custom geolocation field, as they are not supported by the standard Salesforce objects and fields. Reference: Salesforce Integration Architecture Designer Resource Guide, page 20"},{"id":11,"text":"A global financial company sells financial products and services that include the following: 1. Bank Accounts 2. Loans 3. Insurance The company has a core banking system that is state of the art and is the master system to store financial transactions, financial products and customer information. The core banking system currently processes 10M financial transactions per day. The CTO for the company is considering building a community port so that customers can review their bank account details, update their information and review their account financial transactions. What should an integration architect recommend as a solution to enable customer community users to view their financial transactions?","options":["UseSalesforce Connect to display the financial transactions as an external object.","Use Salesforce Connect to display the financial transactions as an external object.","Use Salesforce External Service to display financial transactions in a community lightning page.","Use Iframe to display core banking financial transactions data in the customer community."],"correct":[0],"explanation":"The integration architect should recommend using Salesforce Connect to display the financial transactions as an external object. SalesforceConnect is a feature that allows you to integrate external data sources with Salesforce and access them in real time via external objects. External objects are similar to custom objects, but they store metadata only and not data. You can use external objects to display data from the core banking system in the customer community without copying or syncing the data. You can also use standard Salesforce features, such as reports, dashboards, or global search, with external objects. Salesforce External Serviceis a feature that allows you to import an external schema definitionand generate an Apex wrapper class that can invoke an external service. This is useful for integrating complex business processes or workflows with Salesforce, but not for displaying datain a community lightning page. Iframe is a HTML element that allows you to embed another web page within a web page. This is not a recommended solution for displaying data in a customer community, as it can pose security risks, performance issues, or userinterface problems. References: [Salesforce Connect], [Salesforce External Services], [Using Iframes in Lightning Components]"},{"id":12,"text":"A company in a heavily regulated industry requires data in legacy systems to be displayed in Salesforce user interfaces (UIs). They are proficient in their cloud-based ETL (extract, transform, load) tools. They expose APIs built on their on-premise middleware to cloud and on-premise applications. Which two findings about their current state will allow copies of legacy data inSalesforce? Choose 2 answers","options":["Only on-premise systems are allowed access to legacy systems.","Cloud-based ETL can access Salesforce and supports queues.","On-premise middleware provides APIs to legacy systems data.","Legacy systems can use queues foron-premise integration."],"correct":[1,2],"explanation":"Option B is a correct finding, because it means that the company can use their cloud-based ETL tools to extract data from the legacy systems via the APIs exposed by the on-premise middleware, transform the data as needed, and load the data into Salesforce using queues. Queues are a way of managing asynchronous operations, such as bulk data loading, by placing them in a queue and executingthem when resources are available1. Option C is also a correct finding, because it meansthat the company can expose their legacy data to the cloud via their on-premise middleware APIs. APIs are a way of enabling communication and data exchange between different systems using standard protocols and formats2. By providing APIs to access their legacy data, the company can use any cloud-based tool or application that can consume those APIs toretrieve and manipulate the data. Option A is not a correct finding, because it means that the company cannot access their legacy data from the cloud, which prevents them from using their cloud-based ETL tools or any other cloud-based integration solution. If only on-premise systems are allowed to access the legacy systems, then the company would need to use an on-premise integration tool or application to copy the data to Salesforce. Option D is not a correct finding, because it does not affect the ability to copy legacy data in Salesforce. Queues are useful for on-premise integration, but they are not necessary for copying data to Salesforce. The company can use other methods, such as direct API calls or batch processes, to load data into Salesforce without using queues. References: 1: Queueable Apex | Apex Developer Guide | Salesforce Developers 2: API Basics | SOAP API Developer Guide| Salesforce Developers"},{"id":13,"text":"A company's cloud-based single page application consolidates data local to the application with data from on premise and 3rd party systems. The diagram below typifies the application's combined use of synchronous and asynchronous calls. The company wants to use the average response time of its application's user interface as a basis for certain alerts. For this purpose, thefollowing occurs: 1. Log every call's start and finish date and time to a central analytics data store. 2. Compute response time uniformly as the difference between the start and finish date and time — A to H in the diagram. Which computation represents the end-to-end response time from the user's perspective?","options":["Sum of A to H","Sum of A to F","Sum of A, G, and H","Sum of A and H"],"correct":[3],"explanation":"The end-to-end response time from the user’s perspective is the time elapsed between the user’s request and the user’s receipt of the final response. In the diagram, this corresponds to the sum of A and H, which are the durations of the synchronous calls from the user interface to the cloud-based application and back. The other durations (B to G) are either internal tothe cloud-based application or asynchronous calls that do not affect the user’s perception of response time. Therefore, the correct answer is D, because it represents the sum of A and H."},{"id":14,"text":"A company is planning on sending orders from Salesforce to a fulfillment system. The integration architect has been asked to plan for the integration. Which two questions should the integration architect consider? Choose 2 answers","options":["Can the fulfillment system create new addresses within the Order Create service?","Can the fulfillment system make a callback into Salesforce?","Can the fulfillment system implement a contract-first Outbound Messaging interface?","Is the product catalog data identical at all times in both systems?"],"correct":[1,2],"explanation":"The integration architect should consider whether the fulfillment system can make a callback into Salesforce and whether it can implement a contract-first Outbound Messaging interface. A callback is a wayfor the fulfillment system to send a response or an acknowledgment back to Salesforce after receiving an order1. This can help ensure data consistencyand reliability between the two systems. A contract-first Outbound Messaging interface is an approach where the integration is designed based on a predefined XML schema that defines the structure and content of the messages2. This can help ensure interoperability and compatibility between the two systems. The other two questions are not relevant for the integration architect to consider. The fulfillment system does not need to create new addresses within the Order Create service, as this is a function of Salesforce3. The product catalog data does not need to be identical at all times in both systems, as long as there is a mapping or synchronization mechanism to handle any discrepancies"},{"id":15,"text":"An Integration Architect has built a Salesforce application that integrates multiple systems and keeps them synchronized via Platform Events. What is taking place if events are only beingpublished?","options":["The platform events are published immediately before the Apex transaction completes.","The platform events are published after the Apex transaction completes.","The platform events has a trigger in Apex.","The platform events are being published from Apex."],"correct":[1],"explanation":"Option B is correct because platform events are published after the Apex transaction completes. Platform events are asynchronous messages that are published and consumed by different processes or systems. When an Apex transaction publishes a platform event, the event is not sent immediately, but rather added to a temporary queue. The event is only published and delivered to the subscribers after the Apex transaction commits successfully. This ensures that the event data is consistent with the database state and that the event is not lost in case of a rollback. Option A is incorrect because platform events are not published immediately before the Apex transaction completes. As explained above, platform events are published after the Apex transaction commits successfully. Publishing events before the transaction completes can cause data inconsistency and event loss issues. Option C is incorrect because platform events can have a trigger in Apex, but this is not related to the question of when events are published. A platform event trigger is a special type of Apex trigger that executes when a platform event message is received by Salesforce. A platform event trigger can perform logic based on the event data, such as updating records, sending notifications, or calling external services. A platform event trigger does not affect the publishing of events, but rather the consumption of events. Option D is incorrect because platform events can be published from Apex, but this does not answer the question of when events are published. Platform events can be published from various sources, such as Apex code, Process Builder, Flows, or API calls. Publishing events from Apex requires creating an instance of a platform event object and using the EventBus.publish method to add it to the temporary queue. The actual publishing of events happens after the Apex transaction completes, regardless of the source of the events. References: Publish and Subscribe with Platform Events: Publish Platform Events : Subscribe to Platform Events : Write a Platform Event Trigger : Create and Publish Platform Events in Apex : EventBus Class"},{"id":16,"text":"An Integration Architect has built a solutionusing REST API, updating Account, Contact, and other related information. The data volumes have increased, resulting in higher API calls consumed, and some days the limits are exceeded. A decision was made to decrease the number of API calls using bulk updates. The customer prefers to continue using REST API to avoid architecture changes. Which REST API composite resources should the Integration Architect use to allow up to 200 records in one API call?","options":["SObject Collections","SObject Tree","Batch","Composite"],"correct":[1],"explanation":"SObject Collections is a REST API composite resource that allows you to create, update, or delete up to 200 records in one API call. You can specify the type of operation (create, update, or delete) for eachrecord in the request body, andthe response body will contain the status and IDs of each record. SObject Collections is suitable for bulk operations on records that are not related to each other1. SObject Tree is another REST API composite resource that allows you to create up to 200 records in one API call. However, unlike SObject Collections, SObject Tree requires the records to be related to each other in a hierarchy. You can specify the parent and child records in a JSON tree structure, and the response body will contain the reference IDs and URLs of each record. SObject Tree is suitable forcreating nesteddata in one request2. Batch is a REST API composite resource that allows you to combine up to 25 requests in one API call. Each request can be a different type of operation (query, create, update, delete, etc.) on different objects. The response body will contain the status and results of each request. Batch is suitable for grouping multiple requests into one transaction3. Composite is a REST API composite resource that allows you to execute aseries of REST API requests in one API call. You can use the output of one request as the input of another request using a reference ID. The response body will contain the status and results of each request. Composite is suitable for chaining requests that depend on each other. Therefore, the correct answer is A, because SObject Collections is the only REST API composite resource that allows bulk updates on records that are not related to each other. References: 1: SObject Collections | REST API Developer Guide | Salesforce Developers 2: SObject Tree | REST API Developer Guide | Salesforce Developers 3: Batch | REST API Developer Guide | Salesforce Developers :[Composite | REST API Developer Guide | Salesforce Developers]"},{"id":17,"text":"Northern Trail Outfitters (NTO) uses a custom mobile app to interact with their customers. One of the features of the app are Salesforce Chatter Feeds. NTO wants to automatically post a Chatter item to Twitter whenever the post includes the #thanksNTO hashtag. Which API should an Integration Architect use to meet this requirement?","options":["Connect REST API","REST API","Streaming API","Apex REST"],"correct":[0],"explanation":"The Connect REST API is designed to access Chatter feeds and social data such asusers, groups, followers, and files. By using this API, the integration can query the Chatter posts that include the #thanksNTO hashtag and post them to Twitter using another API. The REST API, Streaming API, and Apex REST are not specific to Chatter and do not provide the same level of functionality and ease ofuse as the Connect REST API2 References: 1: Fire and Forget Pattern 2: Connect REST API"},{"id":18,"text":"What should an Architect recommend to ensure all integrations to the Northern Trail Outfitters company portal useSSL mutual authentication?","options":["Enable My Domain and SSL/TLS.","Enforce SSL/TLS MutualAuthentication.","Generate a Self-signed Certificate.","Generate a CA-signed Certificate."],"correct":[1],"explanation":"This is because SSL/TLS mutual authentication is a security featurethat allows Salesforce to verify the identity of the client that connects to its API endpoint on port 8443. To enable this feature, you need to upload a client certificate to Salesforce and assign the “Enforce SSL/TLS Mutual Authentication” user permission to the users who need to access the company portal. The other options are not sufficient for this scenario because: A, Enable My Domain and SSL/TLS, is a prerequisite for using SSL/TLS mutual authentication, but it does not ensure that all integrations use it. My Domain allows you to customize your Salesforce domain name and SSL/TLS provides encryption for your data in transit. C, Generate a Self-signed Certificate, is a step that you need to do on the client side to create a certificate that can be usedfor SSL/TLS mutual authentication, but it does not ensure that allintegrations use it. You also need to upload the certificate to Salesforce and assign the user permission. D, Generate a CA-signed Certificate, is an alternative to generating a self-signedcertificate, but it also does not ensure that all integrations use SSL/TLS mutual authentication. You still need to upload the certificate to Salesforce and assign the user permission. References: Certificates in Mutual Authentication Configure Your API Client to Use Mutual Authentication Salesforce Mutual Authentication Setup"},{"id":19,"text":"KiA B2C Enterprise Customer has the following use case that involves processing payment from an external payment gateway service in Salesforce. 1. Customer requests Customer Service Representative (CSR) for a Service upgrade. 2. Customer provides credit card details to CSR for payment. 3. CSR submits payment information in Salesforce, and processed in a 4. CSR receives confirmation of payment. 5. CSR upgrades service for customer and confirms Customer. external payment gateway. This use case requires the CSR to obtain confirmation of payment before upgrading the service. The integration with Payment gateway needs to be reliable and monitored for audit purposes. The payment gateway service isan external RESTful service that the B2C Enterprise Customer has subscribed for. What should an Integration Architect recommend for this integration?","options":["Build a custom Apex Callout to external Payment gateway service and provide success message to the CSR, the details of callouts and responses are logged for audit purposes.","Use External Services feature to integrate gateway to Salesforce ensuring real-time updates the CSR and support post payment processes.","Make a callout to the payment gateway through ESB supporting error handling and logging for audit purposes.","Platform events allow integration to payment gateway through the exchange of real-time event data, platform events are scalable and secure."],"correct":[2],"explanation":"Make a callout to the payment gateway through ESB supporting error handling and logging for audit purposes. This solution meets the requirements of integrating with an external RESTful service, ensuring real- time updates to the CSR, and supporting post payment processes. ESB stands for Enterprise Service Bus, which is a software architecture model that allows communication between different applications via a common bus. ESB can handle the callout to the payment gateway service, and provide error handling, logging, routing, transformation,and orchestration capabilities. ESB can also integrate with other systems or services that are involved in the post payment processes, such as billing, invoicing, or reporting. References: Certification - Integration Architect - Trailhead, [Enterprise Integration Patterns]"},{"id":20,"text":"Salesforce users need to read data from an external system via HTTPS request. Which two security methods should an integration architect leverage within Salesforce to secure the integration? Choose 2 answers","options":["Connected App","Named Credentials","Authorization Provider","Two-way SSL"],"correct":[1,3],"explanation":"Named Credentials and Two way SSL are two security methods that an integration architect can leverage within Salesforce to secure the integration with an external system via HTTPS request. Named Credentials are a type of metadata that store authentication settings for accessing external services. They allow you to specify the URL of a service,the authentication protocol, and the credentials for accessing the service. Two way SSL is a type ofmutual authentication that requires both the client and the server to present their certificates to each other. This ensures that both parties are who theyclaim to be, and that the communication is encrypted. Two way SSL can be configured in Salesforce by uploading a certificate and a private key, and associating them with a named credential. References: Certification - Integration Architect - Trailhead, [Named Credentials], [Mutual Authentication]"},{"id":21,"text":"Northern Trail Outfitters (NTO) has hired an Integration Architect to design the integrations between existing systems and a new instance of Salesforce. NTO has the following requirements: 1. Initial load of 2M Accounts, 5.5M Contacts, 4.3M Opportunities, and 45k Products into the new org. 2. Notification of new and updated Accounts andContacts needs to be sent to 3 external systems. 3. Expose custom business logic to 5 external applications in a highly secure manner. 4. Schedule nightly automated dataflows, recipes and data syncs. Which set of APIs are recommended in order to meet the requirements?","options":["Bulk API, Chatter REST API, Apex SOAP API, Tooling API","Bulk API, Chatter REST API, Apex REST API, Analytics REST API","Bulk API, Streaming API, Apex REST API, Analytics REST API","Bulk API, Streaming API, Apex SOAP API, Analytics REST API"],"correct":[2],"explanation":"The set of APIs recommended to meet the requirements are Bulk API, Streaming API, Apex REST API, and Analytics REST API. Bulk API is a RESTful API that allows you to quickly load large amounts of data into Salesforce. Bulk API is suitable for the initial load of millions of records into the new org. Streaming API is a type of streaming event that notifies subscribers of changes to Salesforce records that match a SOQL query. Streaming API is suitable for sending notifications of new and updated Accounts and Contacts to external systems. Apex REST API is an API that allows you to expose custom business logic as RESTful web services that can be accessed by external applications. Apex REST API is suitable for exposing custom business logic in a highly secure manner. Analytics REST API is an API that allows you to access analytics features, such as dashboards, lenses, datasets, and dataflows. Analytics REST API is suitable for scheduling nightly automated dataflows, recipes and data syncs. The other sets of APIs are not suitable for meeting all the requirements. Chatter REST API is an API that allows you to access Chatter feeds and social data such as users, groups, followers, and files. Chatter REST API is not relevant for this scenario. Apex SOAP API is an API that allows you to expose custom business logic as SOAP web services that can be accessed by external applications. ApexSOAP API is less preferable than Apex REST API because it requires more bandwidth and processing time due to the XML format. Tooling API is an API that allows you to build custom development tools or apps for Salesforce applications. Tooling API is not relevant for this scenario."},{"id":22,"text":"Northern Trail Outfitters is planning to perform nightly batch loads into Salesforce from an externalsystem with a custom Java application using the and the CIO is curious about monitoring recommendations for the jobs from the Technical Architect Which two recommendations will help meet the requirements? Choose 2 answers","options":["Write the errorresponse fromthe Bulk API status to a custom error logging object in Salesforce using an Apex trigger and create reports on the object.","Visually monitor in the Salesforce UI using the \"Bulk Data Load Jobs in Salesforce in the setup menu.","Set the Salesforce debug logs level to \"finest\" and add the user Id running the job to monitor in the \"Debug Logs\" in the setup menu.","Use the getBatchInfo method in the Java application to monitor the status of the jobs from the Java application."],"correct":[1,3],"explanation":"Visually monitor in the Salesforce UI using the “Bulk Data Load Jobs” in Salesforce in the setup menu, and use the getBatchInfo method in the Java application to monitor the status of the jobs from the Java application. These two methods can help the Technical Architect to monitor the nightly batch loads into Salesforce from an external system with a custom Java application using the Bulk API. The “Bulk Data Load Jobs” page in Salesforce shows the status, progress, and results of each batch load job. The getBatchInfo method in the Java application returns a BatchInfo object that contains information about a batch, such as its ID, state, number of records processed, and number of errors. References: Certification - Integration Architect - Trailhead, [Bulk API Developer Guide]"},{"id":23,"text":"An Enterprise Customer is planning to implement Salesforce to support case management. Below, is their current system landscape diagram. Considering Salesforce capabilities, what should the Integration Architect evaluate when integrating Salesforce with the current system landscape?","options":["Integrating Salesforce with Order Management System, Email Management Systemand Case Management System.","Integrating Salesforce with Order Management System, Data Warehouse and Case Management System.","Integrating Salesforce with Data Warehouse, Order Management and Email Management System.","Integrating Salesforce with EmailManagement System, Order Management System and Case Management System."],"correct":[2],"explanation":"Integrating Salesforce with Data Warehouse, Order Management and Email Management System is the best option considering Salesforce capabilities. Salesforce can be used as the primary case management system, replacing the existing one. Salesforce can also integrate with the Data Warehouse to provide analytics and reporting on case data. Salesforce can integrate with the Order Management System to access order information related to cases. Salesforce can integrate with the Email Management System to send and receive emails from customers and agents. Reference: Salesforce Integration Architecture Designer Resource Guide, page 16"},{"id":24,"text":"Northern Trail Outfitters wants to improve the quality of call-outs from Salesforce to their REST APIs. For this purpose, they will require all API clients/consumers to adhere to RESTAPI Markup Language (RAML) specifications that include field-level definition of every API request and response payload. RAML specs serve as interface contracts that Apex REST API Clients can rely on. Which two design specifications should the Integration Architect include in the integration architecture to ensure that ApexREST API Clients unit testsconfirm adherence to the RAML specs? Choose 2 answers","options":["Call the Apex REST API Clients in a test context to get the mock response.","Require the Apex REST API Clients to implement the HttpCalloutMock.","Call the HttpCalloutMock implementation from theApex REST API Clients.","Implement HttpCalloutMock to return responses per RAML specification."],"correct":[0,3],"explanation":"The HttpCalloutMock interface allows testing HTTP callouts by returning a predefined response in a test context. By implementing HttpCalloutMockto return responses per RAML specification, the Apex REST API Clients unit tests can confirm that the API requests and responses match the expected format and values. Calling the Apex REST API Clients in a test context to get the mock response is also necessary to verify the adherence to the RAML specs. Calling the HttpCalloutMock implementation from the Apex REST API Clients or requiring the Apex REST API Clients to implement the HttpCalloutMock are not valid options because the HttpCalloutMock interface is implemented by a separate class that is passed as a parameter to the Test.setMock method2 References: 1: Idempotent REST APIs 2: Testing HTTP Callouts by Implementing the HttpCalloutMock Interface"},{"id":25,"text":"An Architect has received a request to prevent employees that leavethe company from accessing data in Salesforce after they are deactivated in the company's HR system. What should an Architect determine before recommending a solution?","options":["Determine inbound integration requirements, then identify frequency.","Determinedata access prevention requirements, then identify frequency.","Determine data volume requirements, then identify the loading schedule.","Determine data access prevention requirements, then identify system constraints."],"correct":[1],"explanation":"The Architect should determine the data access prevention requirements and then identify the system constraints before recommending a solution. The data access prevention requirements are the business rules and policies that define how to prevent deactivated employees from accessing Salesforce data. For example, the Architect should determine whether the HR system can send a notification to Salesforce when an employee is deactivated, or whether Salesforce needs to query the HR system periodically to check the employee status. Thesystem constraints are the technical limitations and challenges that may affect the integration solution. For example, the Architect should determine whether the HR system supports outbound or inbound integration, what are the security and authenticationprotocols, what are the data formats and protocols, and what are the performance and scalability requirements. The frequency of the integration is not as important as the requirements and constraints, as it can be adjusted based on the business needs and technical feasibility. The data volume requirements and loading schedule are not relevant for this scenario, as they are more applicable for data migration or replication scenarios."},{"id":26,"text":"Acustomer imports data from an external system into Salesforce using Bulk API. These jobs have batch sizes of 2000 and are run in parallel mode. The batch fails frequently with the error \"Max CPU time exceeded\". A smaller batch size will fix this error. Which two options should be considered when using a smaller batch size? Choose 2 answers","options":["Smaller batch size may cause record-locking errors.","Smaller batch size may increase time required to execute bulk jobs.","Smaller batch size may exceed theconcurrent API request limits.","Smaller batch size can trigger \"Too many concurrent batches\" error."],"correct":[0,1],"explanation":"The error “Max CPU time exceeded” occurs when a batch of records takes more than 10 minutes to process in the Bulk API. Using a smaller batch size can reduce the processing time and avoid this error. However, there are some trade-offs to consider when using a smaller batch size, such as: Answer A is valid because a smaller batch size may cause record-locking errors if multiple batches try toupdate the same records or records that are related by a lookup or master-detail relationship. This can result in concurrency issues and data inconsistency1 Answer B is valid because a smaller batch size may increase the time required to executebulk jobs, as more batches need to be created and processed sequentially. This can affect the performance and efficiency of the integration1 Answer C is not valid because a smaller batch size may not exceed the concurrent API request limits, as the Bulk API does not count against the concurrent request limit. TheBulk API has its own limit on the number ofbatches that can be queued or in progress, which is 5,000 per rolling 24-hour period2 Answer D is not valid because a smaller batch size can trigger “Too many concurrent batches” error only if the number of batches exceeds the limit of 5,000 per rolling 24-hour period. Thiserror is not related to the parallel mode of the Bulk API, which allows up to 10 batches to be processed simultaneously"},{"id":27,"text":"Universal Containers (UC) is a leading provider of management training globally, UC embarked on a Salesforce transformation journey to allow students to register for courses in the Salesforce community. UC has a learning system that masters all courses and student registration. UC requested a near real-time feed of student registration from Salesforce to the learning system. The integration architect recommends using Salesforce event. Which API should be used for the Salesforceplatform event solution?","options":["Tooling API","Streaming API","O REST AP","SOAP API"],"correct":[1],"explanation":"The API that should be used for the Salesforce platform event solution is Tooling API. Tooling API is a specialized API that exposes metadata used in developer tooling that you can access through REST or SOAP. You can use Tooling API to create, update,or delete platform event definitions and fields4 Streaming API is used to subscribe to platform events and receive notifications when they are published5 References: Platform Events Developer Guide | Salesforce Developers, Define and Publish Platform EventsUnit | Salesforce Trailhead QUESTIONNO: 241 Northern Trail Outfitters is in the final stages of merging two Salesforce orgs but needs to keep the retiring org available for a short period of time for lead management as it is connected to multiple publicweb site forms. The sales department has requested that new leads are available in the new Salesforce instance within 30 minutes. Which two approaches will require the least amount of development effort? Choose 2 answers A Configure named credentials inthe source org. B Use the Composite REST API to aggregate multiple leads in a single call. C Use the tooling API with Process Builder to insert leads in real time. D Call the Salesforce REST API to insert the lead into the target system. Answer: A, B The two approaches that will require the least amount of development effort are configuring named credentials in the source org and using the Composite REST API to aggregate multiple leads in a single call. Named credentials are a type of metadata that store authentication information for accessing external services, such as the target Salesforce org. By using named credentials, you can simplify the code for making callouts and avoid hardcoding credentials or tokens. The Composite REST API is a resource that allows you to execute multiple REST API requests in a single call. You can use the Composite REST API to create, update, or delete up to 25 records in one request. This can reduce the number of API calls and improve performance. References: [Named Credentials], [Composite Resources]"},{"id":28,"text":"Northern Trail Outfitters needs to secure an integration with an external Microsoft Azure API Gateway. What integration security mechanism should be employed?","options":["Configure mutual server authentication with two-way SSL using CA issued certificates.","Configure a connected app with an authorization endpoint of the API gateway and configure OAuth settings.","Use an API only user profile and implement use an external identity provider with federated API access.","Implement Salesforce Shield with Encryption at Rest and generate a tenant secret."],"correct":[1],"explanation":"The OAuth protocol is a standard way to authorizeaccess to web resources. By configuring a connected app with an authorization endpoint of the API gateway, Salesforce can obtain an access token from the API gateway and use it to invoke the external API securely. This avoids the need to manage certificates oruser credentials for authentication2 References: 1: Data Virtualization Pattern 2: OAuth 2.0 WebServer Authentication Flow"},{"id":29,"text":"Sales representatives at Universal Containers (UC) use Salesforce Sales Cloud as their primary CRM. UC owns a legacy homegrown application that stores a copy of customer dataas well. Sales representatives may edit or update Contact records in Salesforce if there is a change. Both Salesforce and the homegrown application should be kept synchronized for consistency. UC has these requirements: 1. When a Contact record in Salesforce is updated, the external homegrown application should be 2. The synchronization should be event driven. 3. The integration should be asynchronous. Which option should an architect recommend to satisfy the requirements?","options":["Leverage Platform Events to publish a custom event message containing changes to the Contact object.","Leverage Change Data Capture to track changes to theContact object and write a CometD subscriber on the homegrown application.","Write an Apex Trigger with the @future annotation.D Use an ETL tool to keep Salesforce and the homegrown application in sync on a regular candence."],"correct":[1],"explanation":"Leverage Change Data Capture to track changes to the Contact object and write a CometD subscriber on the homegrown application. This solution meets the requirements of keeping Salesforce and the homegrown application synchronized for consistency, using event-driven and asynchronous integration. Change Data Capture is a feature that allows you to receive near real-time notifications of changes to Salesforce records, such as inserts, updates, deletes, and undeletes. You can use Change Data Capture to track changes to the Contact object, and publish them as events to an event bus. CometD is a protocol that allows you to subscribe to events from the event bus using a streaming API. You can write a CometD subscriber on the homegrown application, and use it to receive the events and update the customer data accordingly. References: Certification - Integration Architect - Trailhead, [Change Data Capture Developer Guide], [Streaming API Developer Guide]"},{"id":30,"text":"Universal Containers has a requirement for all accounts that do NOT qualify forabusiness extension (Custom field on the account record) for the next month to send a meeting invite to their contacts from the marketing automation system to discuss the next steps. It is estimated there will be approximately 1MilIion contacts per month. What is the recommended solution?","options":["Use Batch Apex.","Use Time-based workflow rule.","Use Process builder.","Use Trigger."],"correct":[1],"explanation":"The recommended solution is to use a time-based workflow rule. A time-based workflow rule is a type of workflow rulethat executes actions at a specific time, such as a certain number of days before or after a record field value. By using a time-based workflow rule, you can send an email alert to the contacts of the accounts that do not qualify for a business extension for the next month, and include a meeting invite in the email. This solution can handle large volumes of data and does not require any custom code. Using batch Apex, process builder, or trigger is not a recommended solution because they are more complex andrequire custom code or configuration. Batch Apex is a way to run large-scale and long-running jobs that operate on many records. Process builder is a tool that lets you automate business processes by creating a process with criteria and actions. Trigger is a type of Apex code that executes before or after database operations, such as insert, update, delete, or undelete."},{"id":31,"text":"When user clicks Check Preferences as part of a Lightning flow in Salesforce, preferences from anexternally hosted RESTful service are to be checked in real-time. The RESTful service has OpenAPI 2.0 JSON definitions, responding in data types of Boolean and string values. Which integration pattern and mechanism should be selected to meet the conditions?","options":["Fire and Forget: Process-driven platform events publishes events on Salesforce Event Bus.","Remote Call-In: Salesforce REST API with REST Composite Resources.","Request-Reply: Enhanced External Services invokes a REST API.","Data Virtualization: Salesforce Connect map data external REST data in external objects."],"correct":[2],"explanation":"The correct answer is C, Request-Reply: Enhanced External Services invokes a REST API. This is because Enhanced External Services allows you to register an external web servicethat has an OpenAPI 2.0 specification and use it as an invocable action in a Lightning flow. This way, you can check the preferences from the external service in real-time and get the response in Boolean and string values. The other options are not suitable for this scenario because: A, Fire and Forget: Process-driven platform events publishes events on Salesforce Event Bus, is a pattern that is used for asynchronous integration, where the sender does not expect a response from the receiver. This is not suitable for checking preferences in real-time. B, Remote Call-In: Salesforce REST API with REST Composite Resources, is a pattern that is used for integrating external applications with Salesforce by calling the Salesforce REST API. This is not suitable forinvoking an external service from a Lightning flow. D, Data Virtualization: Salesforce Connect map data external REST data in external objects, isa pattern that is used for accessing external data without copying or synchronizing it to Salesforce. This is not suitable for checking preferences in real-time or using them in a Lightning flow. References: Enhanced External Services Get Started with External Services Registering external services in salesforce"},{"id":32,"text":"A company wants to standardize exception tracking, handling, and analytics. Given the following actions: 1. Build a companywide logging service hosted on a middleware platform 2. Create case object records for exceptions-based thresholds 3. Change all their Apex Loggers to publish Application Exceptions as custom Platform Events. Which two specifications should the integration architect includein the logging service architecture? Choose 2answers","options":["Receive Application Events through Change Data Capture (CDC).","Create Salesforce Cases using the Salesforce REST, SOAP or Bulk API.","Create Salesforce Cases conditionally using automatic Casecreation rules.","Subscribe to the Application Exceptions using the Salesforce Streaming API."],"correct":[1,2],"explanation":"Creating Salesforce Cases using the Salesforce REST, SOAP or Bulk API is a solution that can allow the logging service to create case records in Salesforce based on the exceptions received from the middleware platform. This way, the logging service can use the standard APIs to interact with Salesforce and leverage the features and functionality of the case object. Creating Salesforce Cases conditionally using automatic Case creation rules is a solution that can allow the logging service to create case records in Salesforce based on predefined criteria and actions. This way, the logging service can reduce the number of API calls and avoid creating unnecessary or duplicate cases. Receiving Application Events through Change Data Capture (CDC) is not a solution that is related to this requirement, as CDC is used for capturing data changes in Salesforce and sending them to external systems, not for receiving data from external systems. Subscribing to the Application Exceptions using the Salesforce Streaming API is also not a solution that is related to this requirement, as Streaming API is used for subscribing to events or notifications from Salesforce, notfor sending data to Salesforce. Reference: Salesforce Integration Architecture Designer Resource Guide, page 25- 26"},{"id":33,"text":"business requires automating the check and update of the phone number type classification (mobile vs. landline) for all in-coming calls delivered to their phone sales agents. The following conditions exist: 1. At peak, their call center can receive up to100,000 calls per day. 2. The phone number type classification is a service provided by an external serviceAPI. 3. Business is flexible with timing and frequency to check and update the records (throughout the night or every 6-12 hours is sufficient). ARemote-Call-In pattern and/or Batch Synchronization (Replication via ETL: System -> Salesforce) are determinedto work with a middleware hosted on custom premise. In order to implement these patterns and mechanisms, which component should an integration architect recommend?","options":["ConnectedApp configured in Salesforce to authenticate the middleware.","IoConfigure Remote Site Settings in Salesforce to authenticate the middleware.","An API Gateway that authenticates requests from Salesforce into the Middleware (ETL/ESB).","Firewall and reverse proxy are required to protect internal APIs and resource being exposed."],"correct":[0],"explanation":"A Connected App is a framework that enables an external application to integrate with Salesforce using APIs and standard protocols, such as OAuth. By configuring a Connected App in Salesforce, the integration can authenticate the middleware and grant access to the Salesforce data and services. This is a secure and flexible solution that does not require configuring Remote Site Settings, API Gateway, or Firewall and reverse proxy2 References: 1: BatchApexErrorEvent 2: Connected Apps"},{"id":34,"text":"Northern Trail Outfitters' ERP is integrated with Salesforce and syncs several million contacts per day. Toprevent specific data from syncing, the integration uses a SOQL query filtered by sharing hierarchy. Which two things should an architect do to improve the performance of the integration? Choose 2 answers","options":["Include non-selective criteria in query filters.","Remove the query filters.","Include selective criteria in query filters.","Remove the sharing restrictions."],"correct":[2,3],"explanation":"Option C is correct because including selective criteria in query filters can improve the performance of the integration. Selective criteria are filters that reduce the number of records that need to be scanned by the query optimizer, such as indexed fields, standard indexes, or custom indexes. Selective criteria can help the query run faster and avoid hitting the query timeout limit or the query row limit. Option D is correct because removing the sharing restrictions can improve the performance of the integration. Sharing restrictions are filters that limit the access to records based on the user’s role, profile, or sharing rules.Sharing restrictions can add complexity and overhead to the query execution, as they require additional joins and calculations. Removing the sharing restrictions can simplify the query and reduce the number of records that need to be processed. Option A is incorrect because including non-selective criteria in query filters can degrade the performance of the integration. Non-selective criteria are filters that do not reduce the number of records that need to be scanned by the query optimizer, such as non-indexed fields, formula fields, or OR conditions. Non-selective criteria can cause the query to run slower and hit the query timeout limit or the query row limit. Option B is incorrect because removing the query filters can degrade the performance of the integration. Query filters are conditions that specify which records to retrieve from the database, such as WHERE clauses or LIMIT clauses. Query filters can help the query run faster and avoid retrieving unnecessary or unwanted data. Removing the query filters can increase the number of records that need to be processed and transmitted by the integration. References: Working with Very Large SOQL Queries: Improve performance with custom indexes using Salesforce Query Plan tool: Querying Data That Respects UserPermissions: How does sharing affect SOQL performance?: SOQL SELECT Syntax : SOQL Best Practices"},{"id":35,"text":"A company that is a leading provider of courses and training delivers courses using third party trainers. The trainer for the company has to be verified from 10 different training accreditation verification agencies before providing training for the company. Each training accreditation agency has its own response time, whichcould take days to confirm a trainer. The company decided to automate the trainer accreditation verification process by integrating to the agencies’ web services. What is the recommended approach to automate this process?","options":["Use salesforce external serviceto make the call out, Salesforce external service should check the verification agencies until the result is verified, then update the trainer status to \"verified\".","Create a trigger on the trainer record to make a Callout to each verification agencies,write business logic to consolidate the verification then update the trainer status to verified\".","Make an apex callout using @future annotation to make the call out to all different agencies. The response should update the trainer status to \"verified\".","Use middleware to handle the call out to the 10 different verification services, the middleware will handle the business logic of consolidating the verification result from t 10 services, then make a call-in to salesforce and update the verification status to \"verified\"."],"correct":[3],"explanation":"Answer D is valid because using middleware to handle the call out to the 10 different verification services is a scalable and reliable solution that can handle the complexity and variability of the integration. The middlewarecan orchestrate the calls to the different web services, consolidate the verification results, and handle any errors or retries. The middleware can then make a call-in to Salesforce and update the verificationstatus to “verified” using an API or a platform event12 Answer A is not valid because using Salesforce external service to make the call out to the 10 different verification services is not a feasible or efficient solution. Salesforce external service is a feature that allows invoking anexternal service from a flow and mapping its inputs and outputs to flow variables. However, this feature requires configuring an Apex action, a named credential, and an external service definition for each web service, which is not a low code solution. Moreover, this feature does not support checking the verification agencies untilthe result is verified, as it only invokes the external service once per flow interview3 Answer B is not valid because creating a trigger on the trainer record to make a callout to each verification agency is not a recommended or robust solution. Triggers are Apex code that execute before or after database events, such as insert, update, or delete. However, triggers cannot make callouts directly, as they are part of a database transaction and must complete quickly. To make a callout from a trigger, an asynchronous process such as a future method or a queueablejob must be used, which adds complexity and overhead to the integration. Moreover, triggershave limits on the number of callouts and asynchronous calls they can make per transaction, which may affect the scalability and reliability of the integration. Answer C is not valid because making an Apex callout using @future annotation to make the call out to all different agencies is not a suitable or reliable solution. The @future annotation allows marking a method for execution at a later time when system resources become available. However, this annotation has several limitations and drawbacks, such as: Future methods cannot return values, so they cannot update the trainer status to “verified” directly. Future methods have limits on the number of callouts andfuture calls they can make per execution, which may affect the scalability and reliability of the integration. Future methods run in their own thread and do not share any static variables or state with other methods, which makes it difficult to consolidatethe verification results from different agencies. Future methods are not guaranteed to execute at a specific time or order, which may affect the timeliness and accuracy of the integration. 1: Orchestration Pattern 2: RemoteProcess Invocation—Request and Reply 3: External Services : Apex Developer Guide: Triggers : Apex Developer Guide: Using Future Methods"},{"id":36,"text":"Northern Trail Outfitters' (NTO) Salesforce org usually goes through 8k-10k batches a day to synch data from external sources. NTO's Integration Architect has received requirements for a new custom object, FooBarc, for which 90M records will need to be loaded into the org. Once complete, 20GB (about 30M records) needs to be extracted to an external auditing system. What should the architect recommend using to meet these requirements in a day?","options":["Insert using Bulk API 2.0 and query using REST API.","Insert and query using Bulk API 1.0.","Insert using Bulk API 1.0 and query using REST API.","Insert and query using Bulk API 2.0. Answer: D Answer D is valid because using Bulk API 2.0 for both inserting and querying data canmeet the requirements of loading and extracting large volumes of data in a day. Bulk API 2.0 is a RESTful API that allows creating, updating, deleting, or querying millions of records asynchronously by uploading or downloading CSV or JSON files. Bulk API2.0 has several advantages over Bulk API 1.0, such as: It does not count against the daily limit of 5,000 batches per rolling 24-hour period. It supports PK Chunking for queries, which automatically splits large data sets into manageable chunks based onthe primary key. It simplifies the job lifecycle and reduces the number of API calls needed to perform a bulk operation. Answer A is not valid because using Bulk API 2.0 for inserting data and REST API for querying data is not an optimal solution for handling large volumes of data in a day. REST API is a synchronous API that allows creating, updating, deleting, or querying individual records or small batches of records using HTTP methods. REST API has several limitations and drawbacks for this use case, such as: It counts against the daily limit of 15,000 API requests per 24-hour period. It does not support PK Chunking for queries, which means that large data sets may exceed the query timeout or heap size limits. It requires more API calls and processing time to perform a bulk operation than Bulk API 2.0. Answer B is not valid because using Bulk API 1.0 for both inserting and querying data is not a feasible solution for handling large volumes of data in a day. Bulk API 1.0 is a SOAP-based API that allows creating, updating, deleting, or querying millions of records asynchronously by uploading or downloading XML or CSV files. Bulk API 1.0 has several limitations and drawbacks compared to Bulk API 2.0, such as: It counts against the daily limit of 5,000 batchesper rolling 24-hour period, which may not be enough to load and extract 90M and 30M records respectively. It does not support JSON format for data files, which may not be compatible with some external systems or applications. It requires more API calls andcomplexity to manage the job lifecycle and handle errors or retries than Bulk API 2.0. Answer C is not valid because using Bulk API 1.0 for inserting data and REST API for querying data is not a suitable or reliable solution for handling large volumes ofdata in a day. As explained above, both Bulk API 1.0 and REST API have limitations and drawbacks that may affect the performance, efficiency, and scalability of the integration. Question #:37 UESTION NO: 56 Northern Trail Outfitters (NTO) uses different shipping services for each of the 34 countries it serves. Services are added and removed frequently to optimize shipping times and costs. Sales Representatives serve all NTO customers globally and need to select between valid service(s) for the customer's country and request shipping estimates from that service. Which two solutions should an architect propose? Choose 2 answers","Use Platform Events to construct and publish shipper-specific events.","Invoke middleware service to retrieve valid shipping methods.","Use middleware to abstract the call to the specific shipping services.","Store shipping services in a picklist that is dependent on a country picklist."],"correct":[1,2],"explanation":"Invoking middleware service to retrieve valid shipping methods is a solution that can allow the sales representatives to select between valid services for the customer’s country. The middleware service can act as a single point of entry for all shipping services and provide routing and transformation capabilities. Using middleware to abstract the call to the specific shipping services is a solution that can allow the sales representatives to request shipping estimates from the selected service. The middleware can hide thecomplexity and heterogeneity of the shipping services and provide mediation and orchestration capabilities. Using Platform Events to construct and publish shipper-specific events is not a solution, as Platform Events are used for event-driven integration,not for web-service integration. Storing shipping services in a picklist that is dependent on a country picklist is not a solution, as it does not address how to request shipping estimates from the shipping services. Reference: Salesforce Integration Architecture Designer Resource Guide, page 16-17"},{"id":38,"text":"Universal containers are planning to implement Salesforce as their CRM system. Currently they have the following systems 1. Leads are managed in a Marketing System. 2. Sales people use Microsoft Outlook to enter contacts, emails and manage activities. 3. Inventory, Billing and Payments are managed in their ERP system. 4. The proposed CRM system is expected to provide Sales and Support people the ability to have a single view of their customers and manage their contacts, emails and activities in Salesforce CRM. What should an Integration Consultant consider to support the proposed CRM system strategy?","options":["Plan for migration of customer and sales data across systems on a regular basis to keep them in sync.","Evaluate current and future data and system usage and then identify potential integration requirements to Salesforce.","Explore Out of box Salesforce connectors for integration with ERP, Marketing and Microsoft Outlook systems.","Propose a middleware system that can support interface between systems with Salesforce."],"correct":[1],"explanation":"The integration consultant should evaluate current and future data and system usage andthen identify potential integration requirements to Salesforce. This approach can help to understand the business needs, data flows, data quality, data volume, data frequency, and data security of the proposed CRM system. Based on this analysis, the integration consultant can design and implement the best integration solution for each system, such as Marketing, ERP, or Outlook. References: [Salesforce Integration Architecture Designer Resource Guide]"},{"id":39,"text":"Which two requirements should the Salesforce Community Cloud support for self-registration and SSO? Choose 2answers","options":["SAML SSO and Registration Handler","OpenId Connect Authentication Provider and Registration Handler","SAML SSO and just-in-time provisioning","OpenId Connect Authentication Provider and just-in-time provisioning"],"correct":[1,2],"explanation":"OpenId ConnectAuthentication Provider and Registration Handler, and SAML SSO and just-in-time provisioning are two requirements that the Salesforce Community Cloud can support for self-registration and SSO. OpenId Connect is a protocol that allows users to authenticatewith an external identity provider and access Salesforce resources. A registration handler is a class that implements the Auth.RegistrationHandler interface and defines the logic for creating or updating a user in Salesforce after authentication. SAML SSOis a protocol that allows users to log in to Salesforce with a single click, using an assertion from an identity provider that confirms the user’s identity. Just-in-time provisioning is a feature that allows Salesforce to create or update a user account based on the information in the SAML assertion. References: Certification - Integration Architect - Trailhead, [OpenID Connect Authentication Providers], [SAML Single Sign-On (SSO) Service for Salesforce]"},{"id":40,"text":"A new Salesforce program has the followinghigh level abstract requirement: Business processes executed on Salesforce require data updates between the internal systems and Salesforce Which three relevant details should a Salesforce Integration Architect seek to specifically solve for Integrationarchitecture needs of the program? Which three relevant details should a Salesforce Integration Architect seek to specifically solve for Integration architecture needs of the program? Choose 3 answers","options":["Source and Target system, Directionality, data volume & transformation complexity long with any middleware that can be leveraged.","Integration skills, SME availability and Program Governance details.","Timing aspects - real-time/near real-time (synchronous or asynchronous), batch; update frequency.","Integration Style Process based, Data based, Virtual integration. E Core functional and non-functional requirements for User Experience design, Encryption needs, Community, and license choices."],"correct":[0,2,3],"explanation":"The correct answer is A, C, and D. These are three relevant details that a Salesforce Integration Architect should seek to specifically solve for Integration architecture needs of the program. These detailscan help the Integration Architect to understand the scope, requirements, and constraints of the integration solution, and to choose the appropriate tools, methods, and patterns. The details are: Source and Target system, Directionality, data volume & transformation complexity along with any middleware that can be leveraged. This detail can help theIntegration Architect to identify the systems involved in the integration, the direction of data flow, the amount and complexity of data to be exchanged, and the middleware or platform capabilities that can facilitate the integration. Timing aspects - real-time/near real-time (synchronous or asynchronous), batch; update frequency. This detail can help the Integration Architect to determine the latency, reliability, and scalability requirements of the integration solution, and to choose the suitable integration protocols and techniques. Integration Style - Process based, Data based, Virtual integration. This detail can help the Integration Architect to select the appropriate integration style that matches the business needs and objectives. Process based integration focuses on orchestrating business processes across systems, data based integration focuses on synchronizing data across systems, and virtual integration focuses on providing a unified view of data across systems. References: Certification - Integration Architect - Trailhead, [Integration Patterns and Practices]"},{"id":41,"text":"An Architect is asked to build a solution that allows a service to access Salesforce through the API. What is the first thing the Architect should do?","options":["Create a new user with SystemAdministrator profile.","Authenticate the integration using existing Single Sign-On.","Authenticate the integration using existing Network-BasedSecurity.","Create a special user solely for the integration purposes."],"correct":[3],"explanation":"Create a special user solely for the integration purposes. This is the first thing that the Architect should do when building a solution that allows a service to access Salesforcethrough the API. Creating a special user for the integration purposes can help to ensure security, accountability, and traceability of the API calls. The special user should have a unique username, password, security token, and profile that grants only thenecessary permissions and access for the integration. The special user should also be assigned to an API- only user license type that prevents logging in to the Salesforce UI. References: Certification - Integration Architect - Trailhead, [User Licenses], [API User Permission]"},{"id":42,"text":"Northern Trail Outfitters (NTO) is looking to integrate three external systems that run nightly data enrichment processes in Salesforce.NTO has both of the following security and strict auditing requirements: 1. The external systems must follow the principle of least privilege, and 2. The activities of the eternal systems must be available for audit. What should an Integration Architect recommend as a solution for these integrations?","options":["A shared integration user for the three external system integrations.","A shared Connected App for the three external system integrations.","A unique integration user for each external system integration.","A Connected App for each external system integration."],"correct":[3],"explanation":"Using a Connected App for each external system integration is a good solution because it can provide security, auditing, and monitoring features for each integration. A Connected App is an application that can connect to Salesforce using APIs and OAuth as an authentication protocol. A Connected App can also enforce policies such as IP restrictions, login hours, and session timeout for each integration. Using a shared integration user for the three external system integrations is not a good solution because it violates the principle of least privilege, as well as makes it difficult to audit the activities of each system. Using a shared Connected App for the three external system integrationsis also not a good solution because it does not allow for granular control and visibility of each integration. Using a unique integration user for each external system integration is not enough to meet the security and auditing requirements, as it does not provide any mechanism for authentication, authorization, or encryption. Reference: Salesforce Integration Architecture Designer Resource Guide, page 20-21"},{"id":43,"text":"The director of customer service at Northern Trail Outfitters (NTO) wants tocapture and trend specific business events that occur in Salesforce in real time. The metrics will be accessed in an ad-hoc manner using an external analytics system. The events that are of interest are: A customer has initiated a product exchange via a Case A customer service rep clicks on the \"Authorize Exchange Product\" menu item on the Case A customer has initiated a subscription cancellation via a Case A customer service rep clicks on the \"Initiate Refund\" menu item on the Case Which two solutions willmeet these business requirements? Choose 2 answers","options":["Case after insert Trigger that executes a callout.","Case Workflow Rule that sends an Outbound Message.","Case after insert Trigger that publishes a Platform Event.","Custom Apex controller that publishes a Platform Event."],"correct":[1,2],"explanation":"Outbound Messaging and Platform Events are both suitable solutions for capturing and sending business events from Salesforce to an external system in real time. Outbound Messaging allows you to specify workflow rules that send SOAP messages with field values to designated endpoints1. Platform Events are secure and scalable messages that contain data and can be published and subscribed to using various APIs2. A trigger or a custom Apex controller can be used to publish platform events from Salesforce34. A callout is not a suitable solution because it is a synchronous request that may fail or timeout, and it also consumes API limits. A contract-first Outbound Messaging interface is not a valid option because Outbound Messaging uses a predefined WSDL that cannot be customized"},{"id":44,"text":"Northern Trail Outfitters needs to make synchronous callouts \"available to promise\"services to query product availability and reserve inventory during customer checkout process. Which two considerations should an integration architect make when building a scalable integration solution? Choose 2 answers","options":["The typical and worst-case historical response times.","The number batch jobs that can run concurrently.","How many concurrent service calls are being placed.","The maximum query cursors open per user on the service."],"correct":[0,2],"explanation":"The typical and worst-case historical response times, and how many concurrent service calls are being placed are two considerations that an integration architect should makewhen building a scalable integration solution for synchronous callouts to “available to promise” services. These two factors can affect the performance, reliability, and availability of the integration solution, as well as the user experience of the customer checkout process. Theintegration architect should design the solution to handle high volumes of service calls, optimize the response times, handle errors and timeouts, and avoid hitting governor limits or service quotas. References: Certification - Integration Architect - Trailhead, [Callout Limits and Limitations], [Integration Patterns and Practices]"},{"id":45,"text":"A new Salesforce program has the following high-level abstract requirement: Business processes executed on Salesforce require data updates between their Internal systems and Salesforce. Which relevant detail should an integration architect seek to specifically solve for integration architecture needs of the program?","options":["Core functional and non-functionalrequirements for User Experience design, Encryption needs, Community and license choices","Integration skills, SME availability, and Program Governance details","Timing aspects, real-time/near real-time (synchronous or asynchronous), batch and update frequency"],"correct":[2],"explanation":"Timing aspects, real-time/near real-time (synchronous or asynchronous), batch and update frequency are relevant details that an integration architect should seek to specifically solve for integration architecture needs of the program. These details help to determine the appropriate integration pattern, technology, and solution for the business requirements. Core functional and non-functional requirements for User Experience design, Encryption needs, Community and license choices are important for the overall program design, but not specific to the integration architecture needs. Integration skills, SME availability, and Program Governance details are also important for the program execution, but not specific to the integration architectureneeds."},{"id":46,"text":"Northern Trail Outfitters (NTO) leverages Sales Cloud for tracking and managing leads, accounts, contacts, and opportunities- Orders and order fulfillment is taken care of by an Order Management System (OMS) in the back-office. When an opportunity has changed it's status to \"Closed/Won\" and there are products attached, the details should be passed to the OMS for fulfillment operations. The callout from Salesforce to the OMS should be synchronous. What should an Integration Architect do to satisfy these requirements?","options":["Write a trigger that invokes an Apex proxy class to make a REST callout to the Order Management System.","Use Process Builder to call an Apex proxy class to make a REST callout to theOrder Management System.","Develop a batch Apex job that aggregates Closed Opportunitiesand makes a REST callout to the Order Management System hourly.","Build a Lightning Component that makes a synchronous Apex REST callout to the Order ManagementSystem when a button is clicked."],"correct":[0],"explanation":"A trigger is a programmatic way of executingsome logic when a record is inserted, updated, deleted, or undeleted in Salesforce. A trigger can invoke an Apex class that contains the code to perform aREST callout to an external system. A REST callout is a way of sending an HTTP request to a service endpoint and receiving a response. A REST callout can be synchronous or asynchronous, depending on whether the Apex code waits for the response before continuing the execution. A synchronous callout is suitable for scenarios where the response is needed immediately, such as order fulfillment1. An Apex proxy class is a class that is generated from a WSDL (Web Service Description Language) document of an external SOAP web service. An Apex proxy class can be used to make a SOAP callout to the external web service, but not a REST callout. A SOAP calloutis another way of sending an HTTP request to a service endpoint and receiving a response, but it uses a different format and protocol than REST2. Process Builder is a declarative tool that allows you to automate business processes by defining criteria and actions. Process Builder can invoke an Apex class that implements the Process.Plugin interface, which allows you to extend the functionality of Process Builder with custom logic. However, Process Builder does not support synchronous callouts, because it runs in the background and does not wait for the response from the external system. Process Builder only supports asynchronous callouts, which are executed after the transaction is committed3. Batch Apex is a way of processing large volumes of data asynchronously by breaking them into smaller batches of records. Batch Apex can be used to perform complex or long-running operations on data, such as data cleansing, archiving, or integration. Batch Apex can make callouts to external systems by implementing the Database.AllowsCallouts interface in the batch class. However, Batch Apex is not suitable for scenarios wherethe callout needs to be synchronous, because it runs in the background and does not wait for the response from the external system. Batch Apex also has some limitations, such as the maximum number of batches in the queue, the maximum number of records per batch, and the maximum number of callouts per batch4. A Lightning Component is a reusable unit of user interface that can be used to build modern web apps with Salesforce. A Lightning Component can make a callout to an external system by using JavaScript code or by invoking an Apex controller class that contains the logic for the callout. A Lightning Component can make a synchronous or asynchronous callout, depending on whether the JavaScript code or Apex code waits for the response before continuing the execution. However, a Lightning Component is not a good choice for scenarios where the callout needs to be triggered by arecord change, such as when an opportunity is closed /won. A Lightning Component requires user interaction, such asclicking a button or loading a page, to initiate the callout5. Therefore, the correct answer is A, because writing a trigger that invokes an Apex class to make a REST callout to the Order Management System is the only option that satisfies the requirements of making a synchronous callout when an opportunity is closed/won. References: 1: Callouts From Triggers | Apex Developer Guide | Salesforce Developers 2: Generate an Apex Class from a WSDL | Apex Developer Guide | Salesforce Developers 3: Invoking Apex from Process Builder | Process Automation Developer Guide | Salesforce Developers 4: Using Batch Apex | Apex Developer Guide | Salesforce Developers 5: Make HTTP Requests from JavaScript Code in Lightning Components | Lightning Aura Components Developer Guide | Salesforce Developers"},{"id":47,"text":"Northern Trail Outfitters needs to use Shield Platform Encryption to encrypt social security numbers in order to meet a business requirement. Which two considerations should anIntegration Architect do prior to the implementation of Shield Platform Encryption? Choose 2 answers","options":["Encrypt the data using the most current key.","Review shield platformencryption configurations.","Encrypt all the data so that it is secure.","Use Shield Platform Encryption as a user authentication or authorization tool."],"correct":[1,2],"explanation":"The considerations that an Integration Architect should do prior to the implementation of Shield Platform Encryption are: Review shield platform encryptionconfigurations. Encrypt all the data so that it is secure. Shield Platform Encryption is a feature that allows you to encrypt sensitive data at rest in Salesforce, such as social security numbers, without compromising critical platform functionality. Before implementing Shield Platform Encryption, you should review the shield platform encryption configurations, such as the encryption key management, the encryption policy, and the encrypted fields and files. You should also encrypt all the data that is subject to encryption, not just the data using the most current key. Encrypting all the data ensures that your data is secure and compliant with your business requirements. Encrypting the data using the most current key is not a valid consideration because Shield Platform Encryption uses a deterministic encryption scheme that does not allow you to rotate or re-encrypt your data with a new key. Using Shield Platform Encryption as a user authentication or authorization tool is not a valid consideration because Shield Platform Encryption is not designed for that purpose. Shield Platform Encryption only encrypts data at rest, not in transit or in use."},{"id":48,"text":"A subscription-based media company's system landscape forces many subscribers to maintain multiple accounts and to login more than once. An Identity and Access Management (IAM) system, which supports SAML and OpenId, was recently implemented to improve their subscriber experience through self-registration and Single Sign-On (SSO). The IAM system must integrate with Salesforce to give new self-service customers instant access to Salesforce Community Cloud. Which two requirements should the Salesforce Community Cloud support for self-registration and SSO? Choose 2 answers","options":["SAML SSO and Registration Handler","OpenId Connect Authentication Provider and Registration Handler","SAML SSO and just-in-time provisioning","OpenId Connect Authentication Provider and just-in-time provisioning"],"correct":[1,2],"explanation":"The Salesforce Community Cloud should support OpenId Connect Authentication Provider and Registration Handler, and SAML SSO and just-in-time provisioning for self-registration and SSO. OpenId Connect is a protocol that allows users to authenticate with an external identity provider and access Salesforce resources. A registration handler is a class that implements the Auth.RegistrationHandler interface and defines how to create new users or update existing users in Salesforce from the information received from the identity provider. SAML SSO is a protocol that allows users to log in to Salesforce with a single credential from an identity provider. Just-in-time provisioning is a featurethat allows creating or updating user accounts in Salesforce based on SAML assertions. References: [OpenID Connect Authentication Providers], [Registration Handler Interface], [SAML Single Sign-On for Communities]."},{"id":49,"text":"Northern Trail Outfitters requires an integration to be set up between one of their Salesforce orgs and anexternal data source us Salesforce Connect. The external data source supports Open Data Protocol. Which three configurations should an Integration Architect recommend be implemented in order to secure requests coming from Salesforce? Choose 3 answers","options":["Configure Identity Type for OData connection.","Configure a Certificate for OData connection.","Configure SpecialCompatibility for OData connection,","Configure CSRF Protection for OData connection.","Configure CSRF Protection on External Data Source."],"correct":[0,1,3],"explanation":"Configuring Identity Type, Certificate, and CSRF Protection for OData connection are threeconfigurations that an Integration Architect should recommend to secure requests coming from Salesforce. Identity Type is used to specify the authentication method for the OData connection, such as Basic Authentication, OAuth 2.0, or Named Principal. Certificate is used to enable SSL/TLS encryption for the OData connection, which protects the data in transit from eavesdropping or tampering. CSRF Protection is used to prevent cross-site request forgery attacks, which exploit the trust between the user and Salesforce by sending malicious requests from another website. Configuring Special Compatibility for OData connection is not a configuration that is related to security, but rather to compatibility issues with different versions or implementations of OData. Configuring CSRF Protection on External Data Source is not a configuration that can be done in Salesforce, but rather on the external data source itself. Reference: Salesforce Connect: Custom Adapters Developer Guide, page 9-10"},{"id":50,"text":"ION NO: 28 Northern Trail Outfitters uses a custom Java application to display code coverage and test results for all of their enterprise applicationsand is planning to include Salesforce as well. Which Salesforce API should an Integration Architect use tomeet the requirement?","options":["SOAP API","Analytics REST API","Metadata API","Tooling API"],"correct":[3],"explanation":"Option D is correct because Tooling API is the Salesforce API that can be used to retrieve code coverage and test results for Apex classes and triggers. Tooling API provides REST and SOAP interfaces to access metadata about code, execute tests, and get test results12 Option A is incorrect because SOAP API is a Salesforce API that can be used to create, retrieve, update, or delete records, but not to get code coverage and test results. SOAP API uses a WSDL file to define the parameters for accessing data through the API3 Option B is incorrect because Analytics REST API is a Salesforce API that can be used to access data from reports and dashboards, but not from code coverage and test results. Analytics RESTAPI provides a programmatic way to interact with analyticsfeatures such as lenses, datasets, and dashboards4 Option C is incorrect because Metadata API is a Salesforce API that can be used to retrieve, deploy, create, update, or delete customization information, such as custom object definitions and page layouts, but not code coverage and test results. Metadata API is mainlyused for development tools or backup tools5 References: 1: Introducing Tooling API 2: Tooling API Examples 3: SOAP API Developer Guide 4: Analytics REST API Developer Guide 5: Metadata API Developer Guide"},{"id":51,"text":"Universal Containers is a global financial company that sells financial products and services. There is a daily scheduled Batch Apex job that generates invoice from a given set of orders. UC requested building a resilient integration for this batch apex job in case the invoice generation fails. What should an integration architect recommend to fulfill the requirement?","options":["Build Batch Retry & Error Handling in the Batch Apex Job itself.","Batch Retry & Error Handling report to monitor the error handling.","Build Batch Retry & Error Handling using BatchApexErrorEvent.","Build Batch Retry & Error Handling in the middleware."],"correct":[2],"explanation":"The BatchApexErrorEvent object allows handling errors that occur during Batch Apex execution. By subscribing to this event,the integration can retry the invoice generation for the failed records or perform other recovery actions. This is a resilient and scalable solution that does not require custom code in the Batch Apex job itself or in the middleware1"},{"id":52,"text":"A company needs to integrate a legacy on premise application that can only support SOAP API. After the Integration Architect has evaluated the requirements and volume, they determined that the Fire and Forget integration pattern will be most appropriate for sending data from Salesforce to the external application and getting response back in a strongly typed format. Which integration capabilities should be used to integrate the two systems?","options":["Outbound Message for Salesforce to Legacy System direction and SOAP API using Enterprise WSDL for the communication back from legacy system to salesforce.","Platform Events for Salesforce to Legacy System direction and SOAP API using Partner WSDL for the communication back from legacy system to salesforce.","Platform Events for Salesforce to Legacy Systemdirection and SOAP API using Enterprise WSDL for the communication back from legacy system to salesforce.","Outbound Message for Salesforce to Legacy System direction and SOAP API using Partner WSDL for the communication back from legacy system to salesforce."],"correct":[0],"explanation":"Outbound Message is a SOAP-based notification service that sends information about changes in Salesforce to a specified endpoint. It is a type of Remote Process Invocation—Fire and Forget integration pattern, which means that Salesforce does not wait for a response from the external system. Outbound Message can be configured using workflow rules or process builder, and it can include a subset of fields of the object that triggered the message. Outbound Message requires the external system to expose a SOAP web service that conforms to the WSDL generated by Salesforce1. SOAP API is a way to access Salesforce dataand functionality using SOAP (Simple Object Access Protocol), which is a standard XML-based protocol for exchanging structured data over the web. SOAP API can be used for both inbound and outbound integration, and it supports two types of WSDL (Web Service Description Language) files: Enterprise and Partner. Enterprise WSDL is a strongly typedWSDL file that is specific to an organization’s Salesforce configuration. It can be used when the client application knows the objects and fields it needs to accessat compile time. Partner WSDL is a loosely typed WSDL file that is generic for any organization. It can be used when the client application needs to access data dynamically at run time2. Therefore, the correct answer is A, because Outbound Message is suitable for Fire and Forget integration from Salesforce to the legacy system, and SOAP API using Enterprise WSDL is suitable for getting response back in a strongly typed format from the legacy system to Salesforce. References: 1: Remote Process Invocation—Fire and Forget | Integration Patterns and Practices | Salesforce Developers 2: SOAP API Developer Guide | SOAP API Basics | Salesforce Developers"},{"id":53,"text":"Universal Containers (UC) is a global financial company. UC support agents would like to open bank accounts on the spot for a customer who is inquiring ab UC products. During opening the bank account process, the agents execute credit checks for the customers through external agencies. At a given time, up to 30 concurrent rewill be using the service for performing credit checks for customers. What error handling mechanisms should be built to display an error to the agent when the credit verification process failed?","options":["In case the verification process is down, Use mock service to send the response to the agent.","Handle verification process error in the Verification Webservice API in case there isa connection issue to the Webservice if it responds with an error.","Handle integration errors in the middleware in case the verification process is down, then the middleware should retry processing the request multiple times.","In case the verification process is down, use fire and forget mechanism instead of request and reply to allow the agent to get the response back when the service is bar online."],"correct":[1],"explanation":"Handle verification process error in the Verification Webservice API in case there is a connection issue to the Webservice or if it responds with an error. This solution ensures that the agent can see an error message when the credit verification process failed, and take appropriate actions. The Verification Webservice API can implementerror handling logic to catch anyexceptions or errors that occur during the callout to the external agencies, and return a meaningful error message to the agent. The agent can then retry the verification process, or escalate the issue to a supervisor. References: Certification - Integration Architect - Trailhead, [Apex Web Services and Callouts]"},{"id":54,"text":"Northern Trail Outfitters wants to use Salesforce as a front end for creating accounts using the lead-to- opportunity process. 1. An order is created in Salesforce when the opportunity is closed and won, but the back-end ERP system is the data master for order, 2. Customer wants to be able to see within Salesforce all the stages of order processing like Order Created, OrderShipped, Order Paid that are within the retention window. Which two message durability considerations should an Integration Architect make when designing a solution to meet these business requirements? Choose 2 answers","options":["When subscribing to Salesforce Event bus, ReplaylD is used with a value of -2 to be able to see old and new events.","High-volume event messages are stored for 24 hours (one day).","When subscribing to Salesforce Event bus, ReplaylD is used with a value of -1 to be able to see new events.","High-volume event messages are stored for 72 hours (three days)."],"correct":[0,3],"explanation":"The message durability considerations that an Integration Architect should make when designing a solution to meet these business requirements are: When subscribing to Salesforce Event bus, ReplayID is used with a value of -2 to be able to see old and new events. High-volume event messages are stored for 72 hours (three days). A Salesforce Event bus is a messaging service that allows you to publish and subscribe to events inSalesforce or from external sources. ReplayID is a property of an event message that indicates its position in the event stream. By using ReplayID with a value of -2, you can replay all events stored in the retention window, which can help you see the stages of order processing in Salesforce. High-volume event messages are a type of event message that can handle large volumes of data and are optimized for performance and scalability. High- volume event messages are stored for 72 hours (three days) in the event bus, which means they can be replayed within that time frame. The other options are not correct. Using ReplayID with a value of -1 means you can only see new events that are published after your subscription. High-volume event messages are not stored for 24 hours (one day), but for 72 hours (three days)."},{"id":55,"text":"An architect decided to use Platform Events for integratingSalesforce with an external system for a company. Which three things should an architect consider when proposing this type of integration mechanism? Choose 3 answers","options":["To subscribe to an event, the integration user in salesforce needs read access to theevent entity.","Salesforce needs to be able to store information about the external system in order toknow which event to send out.","External system needs to have the same uptime in order to be able to keep up with Salesforce Platform Events.","To publish an event, the integration user in salesforce needs create permission on the event entity.","Error handling must be performed by the remote service because the event is effectively handed off to the remote system for further processing."],"correct":[0,3,4],"explanation":"Platform Events are a type of event-driven architecture that allows you to publish and subscribe to events in Salesforce and external systems. To subscribe to an event, the integration user in Salesforce needs read access to the event entity, which defines the schema and properties of the event message. To publish an event, the integration user in Salesforce needs create permission on the event entity, which is a special type of sObject that can be inserted into the platform event queue. Error handling must be performed by the remote service because the event is effectively handed off to the remote system for further processing. Salesforce does not guarantee the delivery or acknowledgment of the event by the external system. The external system should implement its own logic to handle errors, such as retrying failed events, logging errors, or sending notifications. References: Certification - IntegrationArchitect - Trailhead, [Platform Events Developer Guide]"},{"id":56,"text":"An architect recommended using Apex code to make callouts to an external system to process insurance quote. What should the integration architect consider to make sure this is the right option for the integration?","options":["The maximum callouts in a single Apex transaction.","The maximum number of parallel Apex callouts in a single continuation.","The limit on long-running requests (total execution time).","The limit of pending operations in the same transaction."],"correct":[0],"explanation":"The maximum calloutsin a single Apex transaction is one of the factors that should be considered when choosing Apex code to make callouts to an external system. The limit is 100 callouts per transaction, and if this limit is exceeded, a LimitException is thrown. Therefore, the integration architect should evaluate the expected volume and frequency of the callouts and design the solution accordingly2 References: Salesforce API Limits and Usage"},{"id":57,"text":"Northern TrailOutfitters has a requirement to encrypt few of widely used standard fields. They also want to be able to use these fields in workflow rules. Which security solution should an Integration Architect recommend to fulfill the business use case?","options":["CryptographyClass","Data Masking","Classic Encryption","Platform Shield Encryption"],"correct":[3],"explanation":"Option D is correct because Platform Shield Encryption allows you to encrypt standard fields and use them in workflow rules. Platform Shield Encryption uses AES 256-bit encryption to protect sensitive data atrest, while preserving platform functionality12 Option A is incorrect because Cryptography Class is a set of Apex methods for creating digests, message authentication codes, signatures, and encryption. It does not provide a way to encrypt standard fields or use them in workflow rules3 Option B is incorrect because Data Masking is a feature that replaces sensitive data in sandboxes with dummy data. It does not encrypt data in production or allow the use of encrypted data in workflow rules45 Option C is incorrect because Classic Encryption is a feature that encrypts custom fields with 128-bit AES encryption. It does not support standard fields or workflow rules67 References: 1: Salesforce Shield - Data Monitoring & End to End Encryption 2: How Shield Platform EncryptionWorks 3: Crypto Class 4: Data Masking - Anonymize Sensitive Data 5: Secure Your SandboxData with Salesforce Data Mask 6: What’s the Difference Between Classic Encryption and Shield Platform Encryption? 7: Salesforce Classic vs. Salesforce Shield PlatformEncryption: Which One Do You Need?"},{"id":58,"text":"A conglomerate is designing a Lightning Web Component (LWC) to display transactions aggregated from different sources. Their current system landscape is as follows: 1. Transactions are created at anytime through their various on-premise and cloud-based systems. 2. All necessary transactions are replicated to a custom Transaction object in Salesforce. It is updated periodically so it only has a subset of the necessary transactions between updates. 3. Middleware supports publish-subscribe interactions and provides RESTful Enterprise APIs that can retrieve transactions from on-premise and cloud-based systems. The company wants to address a usability concern regarding incomplete data displayed on the LWCcomponent. What should the Integration Architect specify so the LWC will be able to display all the required transactions?","options":["Use the Continuation class to call the Enterprise APIs and then process the response in a callback method.","Let the Lightning Data Service with an ©wire adapter display new values when the custom object records change.","Call the Enterprise APIs directly from the LWC's JavaScript code and redisplay the LWC on receipt of the API response.","Publish a Platform Event, have the middleware subscribe and update the custom object on receipt of Platform Event. Answer: C Question #:59 A large enterprise customer with the following system landscape is planning to implement Salesforce Sales Cloud. The following business processes need to be supported in Salesforce: 1. Sales Consultants should be able to have access to current inventory. 2. Enterprise Resource Planning System(ERP) is the system of record for pricing information. 3. Quotes should be generated in Salesforce with pricing from ERP. 4. Sales Management usesa Enterprise Business Intelligence (BI) toolto view Sales dashboards. 5. Master Data Management (MDM) is the system of record for customers and prospects. 6. Invoices should be accessible in Salesforce. Which systems in the landscape should the IntegrationConsultant consider to be integrated with Salesforce to support the business requirements?","ERP, Invoices system, Data Warehouse and BI Tool","ERP, Inventory, Pricing Engine, Invoices system","ERP, MDM, BI tool and Data Warehouse","ERP, MDM, DataWarehouse, Invoices system Answer: B Question #:60 An Architect is required to integrate with an External Data Source via a Named Credential with an Apex callout due to technical constraints. How is authentication achieved?","Handle authentication with login flows.","Handle authentication in the code.","Connect via Salesforce Connect.","Connect via Communities."],"correct":[1],"explanation":"The authentication is achieved by handling it in the code. A Named Credential is a type of metadata that specifies the URL of a calloutendpoint and its required authentication parameters. An Apex callout is a way to invoke anexternal web service or API from Apex code. When using a Named Credential with an Apex callout, the authentication is handled by the Apex code that invokes the callout. The Apex code can use the HttpRequest class to set the endpoint, method, headers, and body of the callout request, and use the Http class to send the request and receive the response. The Apex code can also use the NamedCredential class to access the properties of the Named Credential, such as the principal type, protocol, and credential type. Handling authentication with login flows is not a valid option because login flows are a way to customize the user login process in Salesforce, not for external web services or APIs. Connecting via Salesforce Connect or Communities is not a valid option because they are not related to Apex callouts or Named Credentials."},{"id":61,"text":"Northern Trail Outfitters is seeking to improve the performance and security of outboundintegrations from Salesforce to on-premise servers. What should the Architect consider before recommending a solution?","options":["External gateway products in use","A Default gateway restrictions","Considerations for using Deterministic Encryption","ShieldPlatform Encryption Limitations"],"correct":[0],"explanation":"Option A is correct because external gateway products in use can affect the performance and security of outbound integrations from Salesforce to on-premise servers. External gateway products are software or hardwaredevices that act as intermediaries between Salesforce and the on-premise servers, such as firewalls, proxies, load balancers, or VPNs. They can have different configurations, features, and limitations that can impact the speed, reliability, and security ofthe data transmission. For example, some external gateway products may require authentication, encryption, or compression of the data, which can add overhead and latency to the integration. Some external gateway products may also have bandwidth or throughput limits, which can affect the scalability and availability of the integration. Therefore, the architect should consider the external gateway products in use before recommending a solution. Option B is incorrect because a default gateway restriction is not a factor that can affect the performance and security of outbound integrations from Salesforce to on-premise servers. A default gateway restriction is a feature that allows administrators to restrict outbound requests from Salesforce to a specific IP address or domain name. This can help prevent unauthorized or malicious requests from Salesforce to external systems. However, this feature does not affect the performance or security of the outbound requests themselves, as it only acts as a filter for the destination of the requests. Option C is incorrect because considerations for using deterministic encryption are not relevant for outbound integrations from Salesforce to on-premise servers. Deterministic encryption is a type of encryption that produces thesame ciphertext for the same plaintext input. This can help preserve some functionality and performance of encrypted data in Salesforce, such as filtering, sorting, and indexing. However, deterministic encryption is not applicable for outbound integrations from Salesforce to on- premise servers, as it is only supported for custom fields and not for standard fields or attachments. Moreover, deterministic encryption does not affect the security of the data transmission itself, as it only encrypts the data atrest in Salesforce. Option D is incorrect because Shield Platform Encryption limitations are not relevant for outbound integrations from Salesforce to on-premise servers. Shield Platform Encryption is a feature that allows administrators to encrypt sensitive data at rest in Salesforce using AES 256-bitencryption. This can help protect data from unauthorized access or theft. However, Shield Platform Encryption limitations are not relevant for outbound integrations from Salesforce to on-premise servers, as they only affect the functionality and performance of encrypted data in Salesforce, such as searching, reporting, or validation rules. Shield Platform Encryption does not affect the security of the data transmission itself, as it only encrypts the data at rest in Salesforce. References: Salesforce Integration Patterns and Practices : Salesforce Integration Guide : Restrict Outbound Requests with a Default Gateway : Deterministic Encryption : Shield Platform Encryption Considerations : Shield PlatformEncryption : Shield Platform Encryption Architecture"},{"id":62,"text":"An integration architect has been tasked with integrating Salesforce with an on-premises system. Due to some established policies, this system must remain on-premises. What should the integration architect use to build a solutionfor this requirement?","options":["Use Salesforce Connect if the database supports Open Database Connectivity (ODBC).","Use Heroku Connect if the data is hosted in Heroku.","Use Salesforce Connect If the database supports Open Data Protocol (OData)."],"correct":[2],"explanation":"UseSalesforce Connect if the database supports Open Data Protocol (OData). Salesforce Connect allows you to integrate external data sources with Salesforce without copying the data into Salesforce. OData is a standard protocol for accessing data from different sources. Use Salesforce Connect if the database supports Open Database Connectivity (ODBC) is incorrect because ODBC is not supported by Salesforce Connect. Use Heroku Connect if the data is hosted in Heroku is incorrect because Heroku Connect is used to synchronize data between Salesforce and a Heroku Postgres database, not an on-premises system."},{"id":63,"text":"A customer of Salesforce has used Platform Events to integrate their Salesforce instance with an external third-party Artificial Intelligence (AI) system. The AI system provides a prediction score for each lead that is received by Salesforce. Once the prediction score is received, the lead information is saved to Platform events for other processes. The trigger on the Platform Events is failing once this was rolled out to Production. What type of monitoring should the IntegrationConsultant have considered to monitor this integration?","options":["Monitor Platform Events created per hour limits across the Salesforce instance.","Set up debug logs for Platform Event triggers to monitor performance.","Validate the Platform Event definition matches leads definition.","Monitor the volume of leads that are created in Salesforce."],"correct":[0],"explanation":"The integration consultant should have considered monitoring the Platform Events created per hour limits across the Salesforce instance. This is because Platform Events have a limit on the number of events that can be published or delivered per hour, depending on the edition and license type. If this limit is exceeded, the trigger on the Platform Events may fail or be delayed. Option B is not correct because debug logs for Platform Event triggers are not useful for monitoring performance. Debug logs are used for troubleshooting issues or errors in the trigger logic, not for measuring the throughput or latency of the events. Option C is not correct because validating the Platform Event definition matches leads definition is not a monitoring task, but a design task. The integration consultant should have ensured that the Platform Event definition matches leads definition before deploying the integration to production. Option D is not correct because monitoring the volume of leads that are created in Salesforce is not relevant for this integration. The volume of leads does not affect the trigger on the Platform Events, as long as the Platform Events created per hourlimits are not exceeded."},{"id":64,"text":"Universal learning (UC) is embarked on Salesforce transformation journey, UC will decommission the legacy CRM system and migrate data to Salesforce. The data migration team asked for a recommendation to optimize the performanceof the data load to Salesforce. Which approach should used to meet the requirement?","options":["Use Bulk API to process jobs in parallel mode.","Contact Salesforce support to schedule performance load.","Use Bulk API to process jobs in serial mode.","Use Bulk API to process jobs in high performance mode."],"correct":[0],"explanation":"This is because Bulk API is a RESTful API that is optimized for loading or deleting large sets of data asynchronously. By processing jobs in parallel mode, you can take advantage of the multiple resources available on the Salesforce platform and speed up the data load. The other options are not suitable for this scenario because: B, Contact Salesforce support to schedule performance load, is not a valid option as Salesforce does not offer such a service. You are responsible for optimizing your own data load performance. C, Use Bulk API to process jobs in serial mode, is a slower option than parallel mode as it processes one batch at a time. This is only recommended when you have dependencies betweenrecords or when you want to preserve the order of records. D, Use Bulk API to process jobs in high performance mode, is not a valid option as there is no such mode in Bulk API. You can only choose between parallel and serial modes. References: Introduction to Bulk API 2.0 and BulkAPI Use Bulk API 2.0 Unit"},{"id":65,"text":"Acustomer is migrating from an old legacy system to Salesforce. As part of the modernization effort, they would like to integrate al existing systems that currently work with their legacy application with Salesforce. Which three constraints and pain-pointsshould an integration architect consider when choosing the integration pattern/mechanism? Choose 3 answers","options":["System types - APIs, File systems, Email","Reporting and usability requirements","Multi-language and multi-currency requirement","Error handling mechanisms","Data Volume and Processing volume"],"correct":[0,3,4],"explanation":"The system types, the error handling mechanisms, and the data volume and processing volume are three constraints and pain-points that an integration architect should consider when choosingthe integration pattern /mechanism. The system types determine what kind of interfaces and protocols are available or required for the integration, such as APIs, file systems, email, etc. The error handling mechanisms ensure that the integration can handleany failures or exceptions gracefully and provide appropriate logging and notification. The data volume and processing volume affect the performance and scalability of the integration, as well as the choice of synchronous or asynchronous methods. Reference: Salesforce Integration Architecture Designer Resource Guide, page 17"},{"id":66,"text":"An enterprise architect has requested the Salesforce Integration architect to review the following (see diagram & description) and provide recommendations after carefully considering all constraints of theenterprise systems and Salesforce platform limits. • About 3,000 phone sales agents use a Salesforce Lightning UI concurrently to check eligibility of a customer for a qualifying offer. • There are multiple eligibility systems that provides this service and are hosted externally. However, their current response times could take up to 90 seconds to process and return (there are discussions to reduce the response times in future, but no commitments are made). • These eligibility systems can be accessed through APIs orchestrated via ESB (MuleSoft). • All requests from Salesforce will have to traverse through customer's API Gateway layer and the API Gateway imposes a constraint of timing out requests after 9 seconds. Which three recommendations should be made? Choose 3 answers","options":["ESB (Mule) with cache/state management to return a request ID (or) response if available from external system.","Recommend synchronous Apex call-outs from Lightning UI to External Systems via Mule and implement polling on API gateway timeout.","Use Continuation callouts to make the eligibility check request from Salesforce from Lightning UI at page load.","When responses are received by Mule, create a Platform Event in Salesforce via Remote-Call-In and use the empAPI in the lightningUI to serve 3,000 concurrent users.","Implement a 'Check Update' button that passes a request ID received from ESB (user action needed)."],"correct":[0,2,4],"explanation":"Option A is correct because using ESB (Mule) with cache/state management can help to handle the long-running requests from Salesforce and return a request ID or a response if available from the external systems. This way, Salesforce does not have to wait for the externalsystems to respond and can avoid the API gateway timeout1 Option C is correct because using Continuation callouts can make the eligibility check request from Salesforce from Lightning UI at page load without blocking the UI thread. Continuation callouts are asynchronous and can handle long-running requestsup to 60 seconds23 Option E is correct because implementing a ‘Check Update’ button that passes a requestID received from ESB can allow the user to manually check the status of the eligibility check request. This can be useful when theresponse is not available within the Continuation timeout limit or when the user wants to refresh the data4 Option B is incorrect because recommending synchronous Apex callouts from Lightning UI to External Systems via Mule and implementing polling on APIgateway timeout can cause performance issues and user frustration. Synchronous Apex callouts block the UI thread and can only handle requests up to 120 seconds. Polling on API gateway timeout can increasethe network traffic and consume the callout limits25 Option D is incorrect because creating a Platform Event in Salesforce via Remote-Call-In and using the empAPI in the lightning UI to serve 3,000 concurrent users can introduce complexity and scalability issues. Platform Events are meant for event-driven architecture and not for request-response scenarios. Remote-Call-In requires an additional license and configuration. empAPI has a limit of 1,000 concurrent subscribers per channel678 References: 1: MuleSoft Documentation 2: Salesforce Documentation 4: Salesforce Trailhead 3: Salesforce Developers Blog 5: Salesforce Documentation 6: Salesforce Documentation 7: Salesforce Documentation 8: Salesforce Documentation"},{"id":67,"text":"Northern Trail Outfitters needs to present shipping costs and estimated delivery times totheir customers. Shipping services used vary by region, and have similar but distinct service request parameters. Which integration component capability should be used?","options":["Enterprise Service Bus to determine which shipping service to use, and transform requests to the necessary format.","Outbound Messaging to request costs and delivery times from Shipper delivery services with automated error retry.","APEX REST Service to implement routing logic to the various shipping service.","Enterprise Service Bus user interface to collect shipper-specific form data."],"correct":[0],"explanation":"Using an Enterprise Service Bus (ESB) to determine which shipping service to use, and transform requests to the necessary format is a good solution because it can provide routing, transformation, mediation, and orchestration capabilities for integrating different services. An ESB can also abstract the complexity and heterogeneity of the services from the client application, which simplifies the integration. Using Outbound Messaging to requestcosts and delivery times from Shipper delivery services with automated error retry is not a good solution because Outbound Messaging is a Salesforce-specific feature that uses SOAP web services, which may not be compatible with all shipping services. UsingAPEX REST Service to implement routing logic to the various shipping service is also not a good solution because it can introduce performance and scalability issues, as well as increase the maintenance cost and complexity of the code. Using Enterprise Service Bus user interface to collect shipper-specific form data is not a valid option because an ESB does not have a user interface component. Reference: Salesforce Integration Architecture Designer Resource Guide, page 16-17"},{"id":68,"text":"A company accepts payment requests 24x7. Once they accept a payment request, their service level agreement (SLA) requires them tomake sure each payment request is processed by their Payment System. They track payment requests using a globally unique identifier created at the Data Entry Point. Their simplified flow is as shown in the diagram. They encounter intermittent update errorswhen two or more processes try to update the same Payment Request record at the same time. Which two recommendations should an integration architect make to improve their SLA and update conflict handling? Choose 2 answers","options":["Middleware should coordinate request delivery and payment processing.","Data Entry Point and Middleware should automatically retry requests.","Payment System should process a payment request only once.","Payment System and Middleware should automatically retry requests."],"correct":[0,2],"explanation":"Middleware should coordinate request delivery and payment processing, and Payment System should process a payment request only once. This solution ensures that each payment request is delivered to the Payment System in a reliable and consistent manner,and avoids duplicate or conflicting updates to the same Payment Request record. Middleware can act as a mediator between the Data Entry Point and the Payment System, and implement logic to handle errors, retries, and acknowledgments. Payment System can usethe globally unique identifier to check if a payment request has already been processed, and avoid processing it again. References: Certification - Integration Architect- Trailhead, [Integration Patterns and Practices]"},{"id":69,"text":"A company's security assessment noted vulnerabilities on the un managed packages in their Salesforce orgs, notably secrets that are easily accessible and in plain text, suchas usernames, passwords, and OAuth tokens used in callouts from Salesforce. Which two persistence mechanisms should an integration architect require to be used to ensure that secrets are protected from deliberate or inadvertent exposure? Choose 2 answers","options":["Encrypted Custom Fields","Named Credentials","Protected Custom Metadata Types","Protected Custom Settings"],"correct":[1,3],"explanation":"Named Credentials and Protected Custom Settings are two persistence mechanisms that can be used to ensure that secrets are protected from deliberate or inadvertent exposure. Named Credentials allow you to specify the URL of a callout endpoint and its required authentication parameters in one definition. Salesforce manages all the authentication for Apex callouts that specify a namedcredential as the callout endpoint, and you don’t have to add more authentication logic in your Apex code. Named Credentials can be defined to provide a secure and convenient way of settingup authenticated callouts, and they can also be usedin Lightning components, Visualforce pages, and flows1. Protected Custom Settings are a type of custom settings that store application-specific data that is hidden from subscribers. They are only accessible by the managed package that created them, and they canbe used to store secrets such as encryption keys, passwords, or tokens. Protected Custom Settings can be accessed by Apex code, formulas, or validation rules within the same namespace as the settings2. Encrypted Custom Fields are not a suitable mechanism for storing secrets, because they are not designed to prevent unauthorized access to sensitive data. Encrypted Custom Fields allow you to encrypt text fields using a standard encryption scheme. The encrypteddata is masked in reports, list views, and search results, but it can still be viewed by users who have the “View Encrypted Data” permission. Encrypted Custom Fields are intended to protect data from unauthorized access by users within your organization, not from external threats or malicious code3. Protected Custom Metadata Types are another type of custom metadata types that store application-specific data that is hidden from subscribers. They are similar to Protected Custom Settings, but they have some advantages such as being deployable using change sets or Metadata API, being accessible by SOQL queries, and being able to reference other metadata types or settings. However, Protected Custom Metadata Types cannot be used to store secrets, because they do not support encryption or masking of sensitive data. Protected Custom Metadata Types are intended to store configuration datathat is specific to your managed package, not secrets that need to be secured. Therefore, the correct answer is B and D, because Named Credentials and Protected Custom Settings are the only persistence mechanisms that can be used to securely store secretsin Salesforce. References: 1: Named Credentials | Apex Developer Guide | Salesforce Developers 2: CustomSettings | Apex Developer Guide | Salesforce Developers 3: Encrypted Fields | Salesforce Help : [Protected Custom Metadata Types | ISVforce Guide | Salesforce Developers]"},{"id":70,"text":"Customer is evaluating Platform Events solution and would like help in comparing/contrasting it with Outbound Message for a real-time / near-real time needs. They expect 3,000 consumers of messages from Salesforce. Which three considerations should be evaluated and highlighted when deciding between the solutions? Choose 3 answers","options":["Both Platform Events and Outbound Message offer declarative means for asynchronous near-real time needs. They aren't best suited for real-time integrations.","In both Platform Events and Outbound Messages, the eventmessages are retried by and delivered in sequence, and only once. Salesforce ensures there is no duplicate message delivery.","Message sequence is possible in Outbound Message but not guaranteed with Platform Events. Both offer very high reliability.Fault handling and recovery are fully handled by Salesforce.","Number of concurrent subscribers to Platform Events is capped at 2,000. An Outbound Message configuration can pass only 100 notifications in a single message to a SOAP end point.","Both Platform Events and Outbound Message are highly scalable. However, unlike Outbound Message, only Platform Events have Event Delivery and Event Publishing limits to be considered."],"correct":[1,3,4],"explanation":"Both Platform Events and Outbound Message offer declarative means for asynchronous near-real time needs. They aren’t best suited for real-time integrations (A). In both Platform Events and Outbound Messages, the event messages are retried by and delivered in sequence, and only once. Salesforce ensures there is no duplicate message delivery (B). Message sequence is possible in Outbound Message but not guaranteed with PlatformEvents. Both offer very high reliability. Fault handling and recovery are fully handled by Salesforce ©. Number of concurrent subscribers to Platform Events is capped at 2,000. An Outbound Message configuration can pass only 100 notifications in a single message to a SOAP end point (D). Both Platform Events and Outbound Message are highly scalable. However, unlike Outbound Message, only Platform Events have Event Delivery and Event Publishing limits to be considered (E). Reference: Salesforce Integration Architecture Designer Resource Guide, page 23"},{"id":71,"text":"Given the diagram below, a Salesforce org, middleware, and Historical data store (with 20million records and growing) exists with connectivity between them Historical records are archived from Salesforce and moved toHistorical Data store (which houses 20M records and growing; fine-tuned to be performant with search queries). Call center agents use Salesforce, when reviewing occasional special cases, have requested access to view the related historical case items thatrelate to submit cases. Which mechanism and patterns are recommended to maximize declarative configuration?","options":["Use ESB tool with Data Virtualization pattern, expose OData endpoint, and then use Salesforce Connect to consume and display the External Objectalongside with the Case object.","C Use an ESB tool with a fire and forget pattern and then publish a platform event for the requested historical data.","Use an ESB tool with Request-Reply pattern and then make a real-time Apex callout to the ESB endpoint to fetch and display component related to Case object","Use an ETL tool with a Batch Data Synchronization pattern to migrate historical data into Salesforce and into a custom object (historical data) related to Case object."],"correct":[0],"explanation":"The Data Virtualization pattern allows Salesforce to access external data sources without storing the data in Salesforce. This reduces the data storage and synchronization costs and enables real-time access to the historical data. Salesforce Connect can be used to consume OData endpoints exposed by the ESB tool and display the external objects as related lists or custom components on the Case object"},{"id":72,"text":"Northern Trail Outfittersis creating a distributable Salesforce package for other Salesforce orgs within the company. The package needs to call into a custor ApexREST endpoint in the central org. The security team wants to ensure a specific integration account is used in the central org that they will authorize after installation of the package. Which three items should an architect recommend to secure the integration in the package? Choose 3 answers","options":["Create an Auth provider in the package and set the consumer key and consumer secret of the connected app in the central org.","Contact Salesforce support and create a case to temporarily enable API access for managed packages.","Create a connected app in the central org and add the callback URL of each org the package is installedin to redirect to after successful authentication.","Use an encrypted field to store the password that the security team enters and use password management for external orgs and set the encryption method to TLS 1.2.","Use the Auth Provider configured and select the identity type as Named Principal with OAuth 2.0 as the protocol and Select Start Authentication Flow on Save."],"correct":[0,2,4],"explanation":"Answer A is valid because creating an Auth provider in the package and setting the consumer key and consumer secretof the connected app in the central org can allow the package to authenticate with the central org using OAuth 2.0. An Auth provider is a configuration that specifies how to connect to an external service that uses a specific identity protocol. A connectedapp is an application that can access Salesforce resources using APIs and standard protocols. The consumer key and consumer secret are credentials that identify the connected app to Salesforce. Answer C is valid because creating a connected app in the central org and adding the callback URL of each org the package is installed in to redirect to after successful authentication can enable the package to obtain an access token from the central org using OAuth 2.0. The callback URL is a parameter that specifies where the user should be redirected after granting or denying permission to access Salesforce resources. The access token is a credential that can be used to invoke the custom Apex REST endpoint in the central org. Answer E is valid because using the Auth Provider configured and selecting the identity type as Named Principal with OAuth 2.0 as the protocol and selecting Start Authentication Flow on Save can initiate the authentication flow when installing the package. The identity type determines how the package accesses Salesforce resources on behalf of users or an application. The Named Principal identity type means that the package uses a single credential, such as a username and password or an access token, to access Salesforce resources for all users.TheStart Authentication Flow on Save option means that the package will prompt the user to enter the credential when saving the Auth Provider configuration. Answer B is not valid because contacting Salesforce support and creating a case to temporarily enable API access for managed packages is not a necessary or recommended action. API access for managed packages is enabled by default and does not require any special permission or configuration from Salesforce support. Moreover, this action does not addressthe security requirement of using a specific integration account in the central org that will be authorized after installation of the package. Answer D is not valid because using an encrypted field to store the password that the security team enters and using password management for external orgs and setting the encryption method to TLS 1.2 is not a secure or reliable solution. An encrypted field is a custom field that encrypts sensitive data at rest and masks it on the user interface. However, this fielddoes not prevent unauthorized access or leakage of data, as it can be decrypted by users who have the View Encrypted Data permission or by Apex code that runs in system mode. Moreover, this field does not support encryption methods such as TLS 1.2, which are used for securing data in transit, not at rest. References: Auth Provider: Connected Apps : OAuth 2.0 Web Server Authentication Flow : Named Credentials as Callout Endpoints : API Access in Packages : Encrypted Fields : Encryption Methods Available in Salesforce 11of30"},{"id":73,"text":"A Salesforce customer is planning to roll out Salesforce for all their Sales and Service staff. Senior Management has requested that monitoring is to be in pla for Operations to notify any degradation in Salesforce performance. How should an integration consultant implement monitoring?","options":["Use Salesforce limits API to capture current API usage and configure alerts for monitoring.","Use APIEVENT to track all user-initiated API calls through SOAP, REST or BULK APIs.","Identify critical business processes and establish automation to monitor performance against established benchmarks.","Request Salesforce to monitor the Salesforce instance and notify when there is degradation in performance."],"correct":[2],"explanation":"The integration consultant should implement monitoring by identifying criticalbusiness processes and establishing automation to monitor performance against established benchmarks. This approach can help detect and diagnose any degradation in Salesforce performance, such as slow response time, high error rate, or low availability. The automation can use various tools and methods, such as platform events, event monitoring, health check, or third-party monitoring services123 References: Proactive Monitoring - Salesforce.com, 17 Free Ways to Monitor Your Salesforce Org | Salesforce Ben, Measure Lightning Experience Performance and Experienced … -Trailhead"},{"id":74,"text":"Universal Containers (UC) owns a variety of cloud-based applications, including Salesforce, alongside several on premise applications. The on-premise applications are protected behind a corporate network with limited outside access to external systems. UC would like to expose data from the on-premise applications to Salesforce for a more unified user experience. The data should be accessible from Salesforce in real-time. Which two actions should be recommended to fulfill this system requirement? Choose 2 answers","options":["Develop an application in Heroku that connects to the on-premisedatabase via an ODBC string and VPC connection.","Develop custom APIs on the company's network that are invokable by Salesforce.","Deploy MuleSoft to the on-premise network and design externally facing APIs to expose the data.","Run a batch job with an ETL tool from an on-premise server to movedata to Salesforce."],"correct":[1,2],"explanation":"The system requirement is to expose data from the on-premise applications to Salesforce for a more unified user experience in real-time. The possible actions that can fulfill this requirement are: Answer B is valid becausedeveloping custom APIs on the company’s network that are invokable by Salesforce can allow Salesforce to access the data from the on-premise applications in real- time. However, this action requires opening a firewall port or using a reverse proxy to allow Salesforce to communicate with the on-premise APIs securely3 AnswerC is valid because deploying MuleSoft to the on-premise network and designing externally facing APIs to expose the data can also enable Salesforce to access the data from the on-premise applications in real-time. MuleSoft is an integration platform that can connect various systems and applications using APIs and standard protocols. MuleSoft can also handle security, orchestration, transformation, and other integration aspects. Answer A is not valid because developing an application in Heroku that connects to the on-premise database via an ODBC string and VPC connection is not a real-time solution. Heroku is a cloud platform that can host web applications and services, but it cannot expose data from an on-premise database directly to Salesforce. To use Herokufor this purpose, an ETL tool or a middleware solution would be needed to synchronize the data between the on-premise database and Heroku, which would introduce latency and complexity to the integration. Answer D is not valid because running a batch job with an ETL tool from an on-premise server to move data to Salesforce is also not a real-time solution. ETL stands for extract, transform, and load, which is a process of moving data from one system to another in batches. ETL toolscan be used for data migration or integration, but they cannot provide real-time access to data from on-premise applications to Salesforce. ETL tools also have limitations on data volume, frequency, and reliability. 1: Bulk API Limits 2: Bulk API Features 3: Integrating with Salesforce using Custom Web Services : MuleSoft Overview : Heroku Overview : ETL Overview"},{"id":75,"text":"ON NO: 54 Universal Containers (UC) uses Salesforce to track the following customer data: 1. Leads, 2. Contacts 3. Accounts 4. Cases Salesforce is considered to be the system of record for the customer. In addition to Salesforce, customer data exists in anEnterprise Resource Planning (ERP) system, ticketing system, and enterprise data lake. Each of these additional systems have their own unique identifier. UC plans on using middleware to integrate Salesforce with the external systems. UC has a requirementto update the proper external system with record changes in Salesforce and vice versa. Which two solutions should an Integration Architect recommend to handle this requirement? Choose 2answers","options":["Locally cache external ID'S at the middleware layer and design business logic to map updates between systems.","Store unique identifiers in an External ID field in Salesforce and use this to update the proper records across systems.","Use Change Data Capture to update downstream systems accordingly when a recordchanges.","Design an MDM solution that maps external ID's to the Salesforce record ID."],"correct":[2,3],"explanation":"Using Change Data Capture (CDC) to update downstream systems accordingly when a record changes is a solution that can handle this requirement by capturingdata changes in Salesforce and sending them to external systems via a publish-subscribe model. This way, the external systems can receive near real-time updates from Salesforce and synchronize their data accordingly. Designing an MDM solution that maps external ID’s to the Salesforce record ID is a solution that can handle this requirement by creating a master data hub that stores and manages the unique identifiers of each system and their relationships. This way, the MDM solution can ensuredata quality,consistency, and accuracy across systems. Locally caching external ID’s at the middleware layer and designing business logic to map updates between systems is not a good solution because it can introduce performance and scalability issues, as well as increase the complexity and maintenance cost of the middleware layer. Storing unique identifiers in an External ID field in Salesforce and using this to update the proper records across systems is not enough to handle this requirement, as it does not address how to update Salesforce with record changes from external systems. Reference: Salesforce Integration Architecture Designer Resource Guide, page 27-28"},{"id":76,"text":"A company needs to be able to send data from Salesforce to a homegrown system behind a corporate firewall. The data needs to be pushed only one way and doesn't need to be sent in real time. The average volume is 2million records per day. What should an integration architect consider when choosing the right option in building the integration between the external system and Salesforce?","options":["Due to high volume of records, number of concurrent requests can hit the limitfor the REST API call to external system.","Due to high volume of records, a third-party integration tool is required to stage records off platform.","Due to high volume of records, the external system will need to use a BULK API Rest endpoint to connect to salesforce.","Due to high volume of records, salesforce will need to make a REST API call to external system."],"correct":[1],"explanation":"Using a third party integration tool to stage records off platform is a solution that can handle the high volume of data and avoid hitting the API limits for the REST API call to the external system. A third party integration tool can also provide features such as data transformation, error handling, and logging. Due to high volume of records, number of concurrent requests can hitthe limit for the REST API call to external system is not a solution, but a problem that needs to be addressed. Due to high volume of records, the external system will need to use a BULK API Rest endpoint to connect to Salesforce is not a solution, as therequirement is to send data from Salesforce to the external system, not vice versa. Due to high volume of records, Salesforce will need to make a REST API call to external system is not a solution, as it does not address how to handle the high volume of data and avoid hitting the API limits. Reference: Salesforce Integration Architecture Designer Resource Guide, page 18-19"},{"id":77,"text":"Northern Trail Outfitters (NTO)has an affiliate company that would like immediate notifications of changes to opportunities in the NTO Salesforce instance. The affiliate company has a CometD client available. Which solution is recommended in order to meet the requirement?","options":["Create a connected app in the affiliate org and select the \"Accept CometD API Requests\".","A Implement a polling mechanism in the client that calls the SOAP API get updated method to get the ID values of each updated record.","Configure External Services to call the subscriber in Apex in the Onchange Trigger event as part of the flow.","Create a PushTopic update event on the Opportunity Object to allow the subscriber to react to the streaming API."],"correct":[3],"explanation":"Streaming API is the best option for sending real-time notifications of changes to Salesforce data. Streaming API uses a publish-subscribe model to push relevant data to subscribers without polling. Streaming API supports PushTopic events, which are based on SOQL queries that define the data changes to listen for. The affiliate company can subscribe to a PushTopic event on the NTO Salesforce org and receive notifications whenever the data that matches the query changes. This way, the affiliate company can be informed of any updates to the opportunities in the NTOSalesforce instance."},{"id":78,"text":"Which two system constraint questions should be considered when designing an integration to send orders from Salesforce to a fulfillment system? Choose 2 answers","options":["What latency is acceptable for orders to reach the fulfillment system?","Which system will validate order shipping addresses?","Can the fulfillment system implement a contract-first Outbound Messaging interface?","Can the fulfillment system participate in idempotent design to avoid duplicate orders?"],"correct":[0,3],"explanation":"The system constraint questions that should be considered when designing an integration to send orders from Salesforce to a fulfillment system are related to the performance, reliability, and scalability of the integration. The latency and idempotency of the integration are important factors that affect these aspects. Therefore, the questions A and D are relevant for the integration design. The question B is related to the business logic of the order validation, which is not a system constraint question. The question C is related to the implementation details of the Outbound Messaging interface, which is not a system constraint question either"},{"id":79,"text":"Northern Trail Outfitters has a registration system that is used for workshopsoffered at its conferences. Attendees use a Salesforce community to register for workshops, but the scheduling systemmanages workshop availability based on room capacity. It is expected that there will be a big surge of requests for workshop reservationswhen the conference schedule goes live. Which integration pattern should be used to manage the influx in registrations?","options":["Remote Process Invocation-Request and Reply","Remote Process Invocation-Fire and Forget","Batch Data Synchronization","RemoteCall-In"],"correct":[2],"explanation":"The Batch Data Synchronization pattern is suitable for this business use case because it allows transferring large volumes of data between Salesforce and the scheduling system in batches. This can handle the surge of requests for workshop registrations without overloading the systems or affecting the performance. The batch process can run at regular intervals or on demand to synchronize the data between the systems1 1: Batch Data Synchronization Pattern"},{"id":80,"text":"Northern Trail Outfitters (NTO) use Salesforce to track leads, opportunities, and to capture order details. However, Salesforce isn't the system that holds or processes orders. After the order details are captured in Salesforce,an order must be created in the remote system, which manages the orders life cylce. The Integration Architect for the project is recommending a remote system that will subscribe to the platform event defined in Salesforce. Which integration pattern shouldbe used for this business use case?","options":["Remote Call In","Request and Reply","Fire and Forget","Batch Data Synchronization"],"correct":[2],"explanation":"The Fire and Forget pattern is suitable for this business use case because it allows sending a message from Salesforceto the remote system without waiting for a response or acknowledgement. This reduces the latency and complexity of the integration and enables asynchronous processing of the orders in the remote system. The platform event defined in Salesforce can be used to publish the order details to the remote system, which can subscribe to the event and create the order accordingly1"},{"id":81,"text":"Universal Containers (UC) is currently managing acustom monolithic webservice that runs on an on-premise server. This monolithic web service is responsible for Point-to-Point (P2P) integrations between: 1. Salesforce and a legacy billing application 2. Salesforce and a cloud-based Enterprise Resource Planning application 3. Salesforce and a data lake. UC has found that the tight interdependencies between systems is causing integrations to fail. What should an architect recommend to decouple the systems and improve performance of the integrations?","options":["Re-write and optimize the current web service to be more efficient.","Leverage modular design by breaking up the web service into smaller pieces for a microservice architecture.","Use the Salesforce Bulk API when integrating back into Salesforce.","Move thecustom monolithic web service from on-premise to a cloud provider."],"correct":[1],"explanation":"A microservice architecture is a way of designing software applications as a collection of loosely coupled services, each of which implements a specific business function.Microservices enable modularity, scalability, and agility in software development, as well as easier testing and deployment. By breaking up the monolithic web service into smaller pieces, each responsible for a single integration, UC can decouple the systems and improve the performance of the integrations. For example, UC can use Salesforce Platform Events to publish and subscribe to events from Salesforce to the legacy billing application, the ERP application, and the data lake. This way, UC can avoid point-to-point integrations and leverage an event-driven architecture that reduces coupling and increases reliability12. The other options are not as effective as option B. Option A does not address the root cause of the tight interdependencies between systems, and may not result in significant performance improvement. Option C may improve the throughput of data loading into Salesforce, but it does not solve the problem of coupling between systems. Option D mayreduce the latency and maintenance costs of the web service, but it does not change the design of the web service or the integrations. References: 1: Microservice Architectures With Sales and Service Cloud - Salesforce Live 2: 6 Fundamental Principles of Microservice Design | Salesforce"},{"id":82,"text":"Universal Containers (UC) currentlyowns a middleware tool and they have developed an API-led integration architecture with three API tiers. The first-tier interfaces directly with the systems of engagement, the second tier implements business logic and aggregates data, while the third-tierinterfaces directly with systems of record. Some of the systems of engagement will be a mobile application, a web application, and Salesforce. UC has a business requirement to return data to the systems of engagement in different formats while also enforcing different security protocols. What should an Integration Architect recommend to meet these requirements?","options":["Enforce separate security protocols and return formats at the first tier of the API-led architecture.","Implement an API gateway that allsystems of engagement must interface with first.","Enforce separate security protocols and return formats at the second tier of the API-led architecture.","Leverage an Identity Provider solution that communicates with the API tiers via SAML"],"correct":[1],"explanation":"Theintegration architect should recommend enforcing separate security protocols and return formats at the first tier of the API-led architecture. The first tier is also known as the experience layer, which is responsible for providing a tailored interface for each system of engagement. By enforcing security and format at this layer, the integration architect can ensure that each system of engagement can access the data in a secure and consistent way, without affecting the other layers. References: [API-ledConnectivity]"},{"id":83,"text":"Northern Trail Outfitters (NTO) has recently changed their Corporate Security Guidelines. The guidelines require that all cloud applicationspass through a secure firewall before accessing on-premise resources. NTO is evaluating middleware solutions to integrate cloud applications with on-premise resources and services. What are two considerations an Integration Architect should evaluate beforechoosing a middleware solution? Choose 2 answers","options":["The middleware solution is capable of establishing a secure API gateway between cloud applications and on-premise resources.","An API gateway component is deployable behind a Demilitarized Zone (DMZ) orperimeter network.","The middleware solution enforces the OAuth security protocol.","The middleware solution is able to interface directly with databases via an ODBC connection string."],"correct":[0,1],"explanation":"An API gateway is a component that acts as a single point of entry for all cloud applications to access on- premise resources and services. It can provide security, routing, transformation, and other features to facilitate integration. An API gateway component should be deployable behind a DMZ or perimeter network, which is a subnetwork that separates the internal network from the external network and provides an additional layer of security. The middleware solution should also be capable of establishing a secure API gateway between cloud applications and on-premise resources, which may involve using protocols such as HTTPS, SSL/TLS, or VPN. The OAuth security protocol is not a requirement for choosing a middleware solution, as it is a standard for authorizing access to resources on behalf of a user. The middleware solution’s ability to interface directly with databases via an ODBC connection string is also not a requirement, as it is a specific feature that may or may not be needed depending on the integration scenario. Reference: Salesforce Integration Architecture Designer Resource Guide, page 14-15"},{"id":84,"text":"An organization needs to integrate Salesforce with an external system and is considering authentication options. The organization already has implemented SAML, using athird-party Identity Provider for integrations between other systems. Which use case can leverage the existing SAML integration to connect Salesforce with other internal systems?","options":["Make formula fields with HYPERLINK() to external web servers more secure.","Make Apex SOAP outbound integrations to external web services more secure.","A Make Apex REST outbound integrations to external web services more secure.","Make an API inbound integration from an external Java client more secure."],"correct":[1],"explanation":"The best use case for leveraging the existing SAML integration to connect Salesforce with other internal systems is to make Apex SOAP outbound integrations to external web servicesmore secure. SAML can be used to authenticate the Salesforce org as the service provider and obtain a session ID from the external system as the identity provider. This session ID can then be used to make SOAP calls to the external web service without exposing any credentials in the Apex code. Option A is not correct because formula fields with HYPERLINK() do not support SAML authentication. Option C is not correct because Apex REST outbound integrations require OAuth or basic authentication, not SAML. Option D is not correct because API inbound integrations from an external Java client require OAuth or basic authentication, not SAML. References: Named Credentials as Callout Endpoints SAML SSO with Salesforce as the Service Provider"},{"id":85,"text":"Universal Containers is a global financial company that sells financialproducts and services including, bank accounts, loans, and insurance. UC uses Salesforce Service cloud to service their customer via calls, live chat. The support agents would open bank accounts on the spot for customers who are inquiring about UC bank accounts. UC Core banking system is the system of record for bank accounts and all accounts opened in salesforce have to be synced in real-time to the core banking system. Support agents need to inform the customers with the newly created bank account ID which has to be generated from the core banking system. Which integration pattern is recommended for this use case?","options":["Use streaming API to generate push topic.","Use outbound message.","Use salesforce platform event.","Use request and reply."],"correct":[3],"explanation":"Using request and reply is the recommended integration pattern for this use case because it allows the support agents to send a request to the core banking system and receive a response with the bank account ID in real- time. This way, the support agents can inform the customers with the newly created bank account ID without anydelay or inconsistency. Using streaming API to generate push topic is not a good solution because it is used for event-driven integration, not for web-service integration. Using outbound message is also not a good solution because it is a Salesforce-specific feature that uses SOAP web services, which may not be compatible with the core banking system. Using Salesforce platform event is also not a good solution because it is used for event-driven integration, not for web-service integration. Reference: Salesforce Integration Architecture Designer ResourceGuide, page 29-30"},{"id":86,"text":"A large consumer goods manufacturer operating in multiple countries isplanning to implement Salesforce for their Sales and Support operations globally. They have the following security requirements: 1. Internal users from each country have to be authenticated with their local active directory. 2. Customers can create their own login or use Google login. 3. Partners have to be authenticated through a central system which is to be determined. 4. Internal users will have access to the central ERP with their credentials maintained in the ERP. 5. Additional internal systemswill be integrated with Salesforcefor Sales and Support business processes. Which three requirements should the integration architect evaluate while designing the integration needs of this project? Choose 3 answers","options":["Evaluate Salesforce solution for customers and for partners, using thirdparty solution for internal users.","Assess security requirements for internal systems and decide Integration methods that support the requirements.","Evaluate the build of a custom authentication mechanism for users in each country and support for customers and partners.","Consider Third party Single Sign On solution supporting all user authentication including customer and partner.","Evaluate Salesforce native authentication mechanism for all users including customers and partners."],"correct":[1,2,4],"explanation":"Therequirements that the integration architect should evaluate while designing the integration needs of this project are: Assess security requirements for internal systems and decide Integration methods that support the requirements. Evaluate the build of acustom authentication mechanism for users in each country and support for customers and partners. Evaluate Salesforce native authentication mechanism for all users including customers and partners. The integration architect should assess the security requirements for internal systems, such as the ERP system and other systems that will be integrated with Salesforce for Sales and Support business processes. The security requirements may include authentication protocols, encryption standards, data access policies, and compliance regulations. The integration architect should then decide on the integration methods that support these requirements, such as SOAP or REST APIs, middleware solutions, or connectors. The integration architect should also evaluate the build of a custom authentication mechanism for users in each country and support for customers and partners. A custom authentication mechanism may be necessary to integrate with the local active directory of each country and provide a seamless user experience. The integration architect should also consider how to support customers and partners who have different authentication options, such as self-registration or Google login. The integration architect should also evaluate Salesforce native authentication mechanism for all users including customers and partners. Salesforce native authentication mechanism is a built-in feature that allows users to log in to Salesforce with their username and password. Salesforce native authentication mechanism may be simpler and easier to implement than a custom or third- party solution, but it may not meet all the security requirements or user preferences. The integration architect should compare the pros and cons of each option and choose the best one for the project. Evaluating Salesforce solution for customers and for partners, using third party solution for internal users is not a valid requirement because it does not address the need to authenticate internal users with their local active directory. Considering third party Single Sign On solution supporting all user authentication including customer and partner is not a valid requirement because it does not address the need to integrate with internal systems and their security requirements."},{"id":87,"text":"Northern Trail Outfitters is planning to create anative employee facing mobile app with the look and feel of Salesforce's Lighting Experience. The mobile ap needs to integrate with their Salesforce org. Which Salesforce API should be used to implement this integration?","options":["Streaming API","REST API","Connect REST API","User InterfaceAPI"],"correct":[3],"explanation":"User Interface API should be used to implement this integration. User Interface API is a RESTful API that allows you to create native mobile apps and custom web apps with the look and feel of Salesforce’s Lightning Experience. User Interface API provides access to metadata, data, and UI components, such as layouts, actions, picklists, and record types. User Interface API also handles security, performance, and compatibility across different devices and browsers. References: Certification - Integration Architect - Trailhead, [User InterfaceAPI Developer Guide]"},{"id":88,"text":"What should an Integration architect consider when recommending Platform Events as an Integration solution?","options":["Event Monitoring Is used to track user activity, such as logins and running reports.","Subscribe to an AssetTokenEvent stream to monitor OAuth 2.0 authentication activity.","When an event definition Is deleted, It's permanently removed and can't be restored."],"correct":[2],"explanation":"Event Monitoring is used to track user activity, such as loginsand running reports, but it is not related to Platform Events. Subscribe to an AssetTokenEvent stream to monitor OAuth 2.0 authentication activity is also not related to Platform Events. When an event definition is deleted, it’s permanently removed and can’t berestored. This means that the event definition and its associated data are no longer available for subscribers or queries."},{"id":89,"text":"A large enterprise customer has decided to implement Salesforce as their CRM. The current system landscape includes the following: 1. An Enterprise Resource Planning (ERP) solution that is responsible for Customer Invoicingand Order fulfillment. 2. A Marketing solution they use for email campaigns. The enterprise customer needs their sales and service associates to use Salesforce to view and log their interactions with customers and prospects in Salesforce. Which system should be the System of record for their customers and prospects?","options":["ERP with all prospect data from Marketing and Salesforce.","Marketing with all customer data from Salesforce and ERP.","Salesforce with relevant Marketing and ERP information.","New CustomDatabase forCustomers and Prospects."],"correct":[2],"explanation":"Option C is correct because Salesforce should be the system of record for their customers and prospects, as it is the CRM solution that the sales and service associates use to view and log their interactions with them. Salesforce can also integrate with the Marketing and ERP solutions to display relevant information from those systems, such as campaign history, invoices, and orders12 Option A is incorrect because ERP is not a suitable system of record for customers and prospects, as it is mainly focused on invoicing and order fulfillment. ERP may not have all the data that the sales and service associates need to interactwith them, such as contact details, preferences, activities, and opportunities. ERP may also have different datamodels and definitions than Salesforce and Marketing, which can cause data quality and consistency issues3 Option B is incorrect because Marketing is not a suitable system of record for customers and prospects, as it is mainly focused on email campaigns. Marketing may not have all the data that the sales and service associates need to interact with them, such as account information, service cases, contracts, and quotes. Marketing may also have different data models and definitions than Salesforce and ERP, which can cause data quality and consistency issues. Option D is incorrect because creating a new custom database for customers and prospects is not a feasible orefficient solution, as it would require additional development, maintenance, and integration costs. It would also create another layer of complexity and potential data duplication in the system landscape. Salesforce already provides a robust and flexibleplatform for managing customer and prospect data, which can be easily customized and integrated with other systems. References: 1: Salesforce CRM - The Definitive Guide 2: SalesforceIntegration Cloud - Connect Any App, Data, or Device 3: What Is ERP? | Oracle : Salesforce vs ERP: What’s the Difference? : Marketing Cloud - Digital Marketing Platform : Salesforce vs Marketing Cloud: What’s the Difference? : Salesforce Platform - Build Apps Fast : Why You Shouldn’t Build Your Own CRM System"},{"id":90,"text":"Northern Trail Outfitters (NTO) has recently implemented middleware for orchestration of services across platforms. The ERP system being used requires transactions be captured near real time at a REST endpoint initiated in Salesforce when creating an order object. Additionally, the Salesforce team has limited development resources and requires a low code solution. Which two options will fulfill the use case requirements? Choose 2 answers","options":["Use Remote Process Invocation fire and forget pattern on inserton the order object using Flow Builder.","Implement a Workflow Rule with Outbound Messaging to send SOAP messages to the designated endpoint.","Implement Change Data Capture on the order object and leverage the replay Idin the middleware solution.","Usea process builder to create a Platform Event, selecting the record type as the Platform Event Name on insert of record."],"correct":[0,2],"explanation":"Answer A is valid because the Remote Process Invocation fire and forget pattern allows sending a message from Salesforceto the ERP system without waiting for a response or acknowledgement. This enables near real-time integration and reduces the latency and complexity of the integration. The Flow Builder is a low code tool that can be used to invoke an Apex actionthat makes the HTTP callout to the REST endpoint on insert of the order object12 Answer C is valid because the Change Data Capture feature allows capturing changes in Salesforce data and publishing them as eventsto a streaming channel. The middleware can subscribe to the channel and receive the events that contain the order details. The replay Id is a unique identifier for each event that can be used by the middleware to resume from the last processed event in case of failures or interruptions. This ensures reliable and consistent integration between Salesforce and the ERP system3 Answer B is not valid because the Outbound Messaging feature uses SOAP messages to send notifications to external systems based on workflow rules. This requires configuring a SOAP endpoint and a WSDL file for the ERP system, which may not be compatible with the REST endpoint requirement. Moreover, Outbound Messaging does not guarantee delivery or order of messages, which may affect the accuracy and timeliness of the integration. Answer D is not valid because the Platform Event feature allows publishing custom events from Salesforce to external systems or vice versa using a publish/subscribe model. However, this requires creating a platform event object and defining its fields and permissions, which may notbe a low code solution. Moreover, using the record type as the platform event name may not be a valid or meaningful option, as record types are used to define different business processes or user interfaces for an object. 1:Remote Process Invocation—Fire and Forget 2: Invoke an Apex Class That Makes a Callout 3: Change Data Capture : Replay Events Using Middleware : Outbound Messaging Implementation Guide : Platform Events Developer Guide"},{"id":91,"text":"The sales Operations team at Northern Trail Outfitters imports new leads each day. An integrated legacy territory management system assigns territories toleads before Sales team members can work on them. The current integration often experiences latency issues. Which two recommendations should an Architect make to improve the integration performance? Choose 2 answers","options":["Reduce batch size of asynchronous BULK API.","Reduce batch size of synchronous BULK API.","Legacy system should submit in serial mode.","Legacy system should submit in parallel mode."],"correct":[0,3],"explanation":"Reducing the batch size of asynchronous BULK API and submitting the legacy system in parallel mode are two recommendations that can improve the integration performance. The BULK API is designed to handle large-scale data loads, but it can also cause latency issues if the batch size is too large or the network bandwidth is insufficient. Reducing the batch size can help to avoid timeouts and improve throughput. Submitting the legacy system in parallel mode can also speed up the integration process by allowing multiple batches to be processed concurrently, as long as there are no dependencies or conflicts between them. Reference: Salesforce Integration Architecture Designer Resource Guide, page 21"},{"id":92,"text":"A largeB2C customer is planning to implement Salesforce CRM to become a Customer centric enterprise. Below, is their current system landscape diagram. The goals for implementing Salesforce follows: 1. Develop a360 view of customer 2. Leverage Salesforce capabilities for Marketing, Sales and Service processes 3. Reuse Enterprise capabilities built for Quoting and Order Management processes Which three systems from the current system landscape can be retired withthe implementation of Salesforce? Choose 3 answers","options":["Order Management System","Case Management System","Sales Activity System","Email Marketing System","Quoting System"],"correct":[1,2,3],"explanation":"The three systems from the current system landscape that can beretired with the implementation of Salesforce are Case Management System, Sales Activity System, and Email Marketing System. These systems can be replaced by Salesforce Service Cloud, Sales Cloud, and Marketing Cloud respectively, which provide similar orbetter capabilities for managing cases, sales activities, and email campaigns. Option A is not correct because Order Management System cannot be retired with the implementation of Salesforce. The company wants to reuse their existing enterprise capabilities for order management processes, which are likely to be complex and customized. Salesforce does not provide a native order management solution for B2C commerce scenarios. Option E is not correct because Quoting System cannot be retired with the implementation of Salesforce. The company wants to reuse their existing enterprise capabilities for quoting processes, which are likely to be complex and customized. Salesforce does not provide a native quoting solution for B2C commerce scenarios."},{"id":93,"text":"An enterprise customer that has more than 10 million customers have the following systems and conditions in their landscape:","options":["Enterprise Billing System (EBS) - All customer's monthly billing is generated by this system.","Enterprise Document Management System (DMS) Billsmailed to customers are maintained in the Document Management system.","Salesforce CRM (CRM)- Customer information, Sales and Support information is maintained in CRM."],"correct":[1,2],"explanation":"The integration consultant should consider the following authorization and authentication needs while integrating the DMS and ESB with Salesforce: Users should be authorized to view information specific to the customer they are servicing without a need to search for customer. This means that the integration should provide a seamless and contextual access to the customer billing information and generated bills from the DMS and ESB, based on the customer record or case that the user is working on in Salesforce. Consider Enterprise security needs for access to DMS and ESB. This means that the integration should comply with the security policies and standards of the Enterprise, such as encryption, auditing, logging, monitoring, etc. Users should be authenticated into DMS and ESB without having to enter username and password. This means that the integration should use a single sign-on (SSO) mechanism that allows users to access multiple systems with one login credential, such as OAuth or SAML. References: [Authorization Through Connected Apps and OAuth 2.0], [Single Sign-On for Desktopand Mobile Applications using SAML and OAuth]"},{"id":94,"text":"Northern Trail Outfitters (NTO) uses Salesforce to track leads, opportunities andorder details that convert leads to customers. However, Orders are managed by an external (remote) system. Sales representatives want to view and update real-time order information in Salesforce. NTO wants the data to only persist in the external system. Which type of Integration shouldan architect recommend to meet this business requirement?","options":["Data Visualization","Data Synchronization","Process Orchestration","Batch Processing"],"correct":[0],"explanation":"Data Visualization is the type of integration that allows users to view real-time data from anexternal system without storing it in Salesforce. This can be achieved by using Lightning Web Components, Visualforce pages, or Canvas apps that display data from the external system using APIs or web services. Data Synchronization, Process Orchestration,and Batch Processing are types of integration that involve moving data between systems, which is not required by the business requirement. Reference: Salesforce Integration Architecture Designer Resource Guide, page 9"},{"id":95,"text":"A developer has been tasked by the integration architect tobuild a solution based on the Streaming API. The developer has done some research and has found there are different implementations of the events in Salesforce (Push Topic Events, Change Data Capture, Generic Streaming, Platform Events), but is unsure ofto proceed with the implementation.The developer asks the system architect for some guidance. What should the architect consider when making the recommendation?","options":["Push Topic Event can define a custom payload.B Change Data Capture does not have record access support.","Change Data Capture can be published from Apex.","Apex triggers can subscribe to Generic Events."],"correct":[1],"explanation":"Push Topic Events can define a custom payload, which is one of the factors that should be considered when choosing a Streaming APIimplementation. Push Topic Events are based on SOQL queries that define the data changes to be notified of. You can specify the fields to be returned in the event message by using the SELECT clause in the query. This allows you to customize the payload according to your needs2 References: Create a PushTopic"},{"id":96,"text":"Which WSDL should an architect consider when creating an integration that might be used for more than one salesforce organization and different metadata?","options":["Corporate WSDL","Partner WSDL","SOAP API WSDL","Enterprise WSDL"],"correct":[1],"explanation":"The Partner WSDL is the best option for creating an integration that might be used for more than one Salesforce organization and different metadata.The Partner WSDL is loosely typed and can reflect against any configuration of Salesforce. It is static and does not change if modifications are made to an organization’s Salesforce configuration. Therefore, it is more flexible and adaptable than the Enterprise WSDL, which is strongly typed and bound to a specific configuration of Salesforce1 References: Differences between Salesforce provided WSDL files"},{"id":97,"text":"NorthernTrail Outfitters needs to send order and line items directly to an existing finance application webservicewhen an order if fulfilled. It is critical that each order reach the finance application exactly once for accurate invoicing. What solution should anarchitect propose?","options":["Trigger invokes Queueable Apex method, with custom error handling process.","Triggermakes @future Apex method, with custom error handling process.","Button press invokes synchronous callout, with user handling retries in case of error","Outbound Messaging, which will automatically handle error retries to the service."],"correct":[0],"explanation":"Trigger invokes Queueable Apex method, with custom error handling process. Queueable Apex allows you to run asynchronous jobs that can make callouts to external web services. You can use custom error handling logic to handle any failures and retry the callouts if necessary. You can also use Database.Stateful interface to maintain state across transactions and ensure that each order is sent exactly once. This solution meets the requirements of sending order and line items directly to an existing finance application web service when an order is fulfilled, and ensuring that each order reaches the finance application exactly once for accurate invoicing. References: Certification - Integration Architect - Trailhead, [Queueable Apex], [Making a Web Service Callout from a Queueable Apex Job]"},{"id":98,"text":"Only authorized users are allowed access to the EBS and the Enterprise DMS. Customers call Customer Support when they need clarification on their bills. Customer Support needs seamless access to customer billing information from theE and view generated bills from the DMS. Which three authorization and authentication needs should an integration consultant consider while integrating the DMS and ESB with Salesforce? should an integration consultant consider while integrating the DMS andESB with Salesforce? Choose 3 answers","options":["Users should be authorized to view information specific to the customer they are servicing without a need to search for customer.","Identify options to maintain DMS and EBS authentication and authorization detailsin Salesforce.","Consider Enterprise security needs for access toDMS and EBS.","Consider options to migrate DMS and EBS into Salesforce.","Users should be authenticated into DMS and EBS without having to enter username and password."],"correct":[0,2,4],"explanation":"The integration consultant should consider the following authorization and authentication needs while integrating the DMS and ESB with Salesforce: Users should be authorized to view information specific to the customer they are servicing without a need to search for customer. This means that the integration should providea seamless and contextual access to the customer billing information and generated bills from the DMS and ESB, based on the customer record or case that the user is working on in Salesforce. Consider Enterprise security needs for access to DMS and ESB. This means that the integration should comply with the security policies and standards of the Enterprise, such as encryption, auditing, logging, monitoring, etc. Users should be authenticated into DMS and ESB without having to enter username and password. This means that the integration should use a single sign-on (SSO) mechanism that allows users to access multiple systems with one login credential, such asOAuth or SAML34 References: Authorization Through Connected Apps and OAuth 2.0, Single Sign-On for Desktop and Mobile Applications using SAML and OAuth"},{"id":99,"text":"A US business-to-consumer (B2C) company is planning to expand to Latin America. They project an initial Latin American customer base of about one million, and a growth rate of around 10% every year for the next 5 years. They anticipate privacy and dataprotection requirements similar to those in the European Union to come into effect during this time. Their initial analysis indicates that key personal data is stored in the following systems: 1. Legacy mainframe systems that have remained untouched for years and are due to be decommissioned. 2. Salesforce Commerce Cloud Service Cloud, Marketing Cloud, and Community Cloud. 3. The company's CIO tasked the integration architect with ensuring that they can completely delete their Latin American customer'spersonal data on demand. Which three requirements should the integration architect consider? Choose 3 answers","options":["Manual steps and procedures that may be necessary.","Impact of deleted records on system functionality.","Ability to delete personal data inevery system.","Feasibility to restore deleted records when needed.","Ability to provide a 360-degree view of the customer."],"correct":[1,2,4],"explanation":"The integration architect should consider the impact of deleted records on system functionality, the ability to delete personal data in every system, and the ability to provide a 360-degree view of the customer. These are important requirements for ensuring that the company can comply with the privacy and data protection regulations, as well as deliver a customer-centric service. Option A is not correct because manual steps and procedures are not desirable for deleting personal data on demand. The integration architect should aim for an automated and reliable solution that minimizes human intervention and errors. Option D is not correct because restoring deleted records when needed may violate the privacy and data protection regulations, as well as the customer’s consent. The integration architect should ensure that the deletion of personal data is permanent and irreversible."},{"id":100,"text":"Universal Containers (UC) is a large printing company that sells advertisement banners. The company works with third-party agents on banner initial design concepts. The design files are stored in an on-premise file store that can be accessed by UC internal users and the third party agencies. UC would like to collaborate with the 3rd part agencies on the design files and allow them to be able to view the design files in the community. The conceptual design files size is 2.5 GB. Which solution should an integration architectrecommend?","options":["Create a lightning component with a Request and Reply integration pattern to allow the community users to download the design files.","Define an External Data Source and use Salesforce Connect to upload the files to an external object. Linkthe external object using Indirect lookup.","Create a custom object to store the file location URL, when community user clicks on the file URL, redirect the user to the on-prem system file location.","Use Salesforce Files to link the files to Salesforcerecords and display the record and the files in the community."],"correct":[2],"explanation":"The best solution for this scenario is to use a custom object to store the file location URL and redirect the community user to the on-premise file store when they click on the URL.This way, the community user can access the large design files without having to download them or use any external data source. Option A is not feasible because the Request and Reply integration pattern is not suitable for large files and would cause performance issues. Option B is not correct because Salesforce Connect cannot upload files to an external object, only data. Option D is not possible because Salesforce Files has a limit of 2 GB per file, and the design files are 2.5 GB in size. References: Salesforce Connect Developer Guide Salesforce Files Developer Guide"},{"id":101,"text":"Which three considerations should an Integration Architect consider when recommending Platform Event as a Integration solution? Choose 3 answers","options":["Inability to query event messages using SOQL","Subscribe to an AssetToken Event streamto monitor OAuth2.0 authentication activity. C","Inability to create a Lightning record page for platform events.","When you delete an event definition, it's permanently removed and can't be restored.","You can use Event Monitoring to track user activity, such as logins and running reports."],"correct":[0,2,3],"explanation":"These are three considerations that an Integration Architect should consider when recommending Platform Event as an Integration solution. Platform Events are a type of event-driven architecture thatallows you to publish and subscribe to custom events in Salesforce and external systems. Platform Events have some limitations and features that an Integration Architect should be aware of, such as: Inability to query event messages using SOQL. Platform Events are not stored in Salesforce database, but in an event bus that retains them for 24 hours. You cannot use SOQL to query event messages, but you can use the EventLogFile object or the Event Monitoring feature to access them. Inability to create a Lightning record page for platform events. Platform Events are not standard or custom objects, but a special type of sObject that can be inserted into the event bus. You cannot create a Lightning record page for platform events, but you can use Apex triggers, Lightning web components, or CometD clients to subscribe to them. When you delete an event definition, it’s permanently removed and can’t be restored. An event definition is a metadata type that defines the schema and properties of a platform event message.You can create or modify event definitions in Salesforce Setup or using Metadata API. However, if you delete an event definition, it’s permanently removed from your org and can’t be restored. You also lose access to any event messages that use that eventdefinition. References: Certification - Integration Architect - Trailhead, [Platform Events Developer Guide]"},{"id":102,"text":"An Integration Developer is developing an HR synchronization app for a client. The app synchronizes Salesforce record data changes with an HR system that's external to Salesforce. What should the integration architect recommend to ensure notifications are stored for up to three days if data replication fails?","options":["Change Data Capture","Generic Events","Platform Events","Callouts"],"correct":[0],"explanation":"Change Data Capture is a feature that enables you to receive near-real-time changes of Salesforce records, including create, update, delete, and undelete operations. Change Data Capture retainschange events in the event bus for up to three days, so you can resume data replication from the point of failure. Change Data Capture also provides a consistent and reliable way to synchronize data changes with external systems, without the need for custom triggers or code. Reference: Salesforce Integration Architecture Designer Resource Guide, page 24"},{"id":103,"text":"Universal Containers (UC) is a leading provider of managementtraining globally, UC requested students course registration data generated from the Salesforce student community to be synced with the learning management system (LMS). Any update to the course registration data needs to be reflected in the LMS. Which integration mechanism should be used to meet the requirement?","options":["Change Data Capture (CDC)","Platform Event","Streaming API","Outbound Message"],"correct":[0],"explanation":"Change Data Capture (CDC) is the best option to meet the requirement. CDC is a type of streaming event that notifies subscribers of changes to Salesforce records, such as creation, update, delete, and undelete operations. By subscribing to CDC events for the course registration data, the LMS can receive real-time updates and sync with Salesforce. Streaming API is another type of streaming event that notifies subscribers of changes to Salesforce records that match a SOQL query. However, Streaming API has some limitations, such as not supporting all SOQL features and not capturing delete and undelete operations. Platform Event is a type of streaming event that delivers custom notifications within the Salesforce platform or from external sources. Platform Event is not suitable for this requirement because it is not tied to Salesforce records and requires custom logic to publish and subscribe. Outbound Message is a workflow action that sends SOAP messages with field values to designated endpoints. Outbound Message is not suitable for this requirement because it does not support complex data types, such as sObjects or collections."},{"id":104,"text":"A company that is a leading provider of training delivers courses to students globally. The company decided touse customer community in order to allow Studer to log in to the community, register for courses and pay course fees. The company has a payment gateway that takes more than 30 seconds to process the pays transaction. Students would like to get the paymentresult in real-time so in case an error happens, the students can retry the payment process. What is the recommended integration approach to process payments based on this requirement?","options":["Use platform event to process payment to the payment gateway.","Use continuation to process payment to the payment gateway.C Use change data capture to process payment to the payment gateway.","Use request and reply to make an API call to the payment gateway."],"correct":[1],"explanation":"The continuation class in Apex allows you to makelong-running requests to external web services and process their responses in a callback method. This is useful for scenarios where the response time exceeds the normal Apex limits, such as 10 seconds for synchronous transactions or 120 seconds for asynchronous transactions. By using continuation, you can avoid hitting these limits and provide a better user experience1 References: Callout Limits and Limitations"},{"id":105,"text":"Northern Trail Outfitters has recently experienced intermittent network outages in its call center. When network service resumes, Sales representatives have inadvertently created duplicate orders in themanufacturing system because the order was placed but the return acknowledgement was lost during the outage. Which solution should an architect recommend to avoid duplicate order booking?","options":["Use Outbound Messaging to ensure manufacturing acknowledges receipt of order.","Use scheduled apex to query manufacturing system for potential duplicate or missing orders.","Implement idempotent design and have Sales Representatives retry order(s) in question.","Have scheduled Apex resubmit orders that do not have asuccessful response."],"correct":[2],"explanation":"Idempotent design means that the same request can be repeated multiple times without changing the outcome. This is useful for avoiding duplicate orders in case of network failures or timeouts. By implementing idempotent design, the sales representatives can retry the order(s) in question without creating duplicates in the manufacturing system. Outbound messaging is not a reliable solution because it does not guarantee delivery or acknowledgement of messages. Scheduled apex is not a real-time solution and may not catch all the duplicate or missing orders"},{"id":106,"text":"The URL for an external service has been changed without prior notice. The service provides up to date money exchange rates that is accessed severaltimes from Salesforce and is a business critical function for end users. Which two solutions shouldan Integration Architect recommend be implemented to minimize potential downtime for users in this situation? Choose 2 answers","options":["Named Credentials","RemoteSite Settings","Content Security Policies","Enterprise ESB"],"correct":[0,3],"explanation":"The best solutions for minimizing potential downtime for users in this situation are to use named credentials and an enterprise ESB. Named credentials allow you to store the URLand authentication parameters of an external service in one definition, and then use it in Apex callouts or declarative integrations. If the URL of the external service changes, you only need to update the named credential once, and all the integrations that use it will be updated automatically. An enterprise ESB is a middleware layer that acts as a broker between different systems and services. It can provide routing, transformation, orchestration, and monitoring capabilities for integrations. If the URL of an external service changes, you only need to update the ESB configuration once, and all the integrations that use it will be updated automatically. Option B is not correct because remote site settings only allow you to specify the domains that are allowed for callouts from your org. They do not store any authentication parameters or provide any routing or transformation capabilities. Option C is not correct because content security policies are used to control what resources can be loaded on a Visualforce or Lightning page. They do not store any authentication parameters or provide any routing or transformation capabilities. References: NamedCredentials Streamline and Secure: Mastering Named Credentials in Salesforce"},{"id":107,"text":"NorthernTrail Outfitters has had an increase in requests from other business units to integrate opportunity information with other systems from Salesforce. The developers have started writing asynchronous @future callouts directly into the target systems. The CIOis concerned about the viability of this approach scaling for future growth and has requested a solution recommendation. What should be done to mitigate the concerns that the CIO has?","options":["Implement an ETL tool and perform nightly batch data loads to reducenetwork traffic using last modified dates on the opportunity object to extract the right records.","Develop a comprehensive catalog of Apex classes to eliminate the need for redundant code and use custom metadata to hold the endpoint information for eachintegration.","Refactor the existing ©future methods to use Enhanced External Services, import Open API 2.0 schemas and update flows to use services instead of Apex.","Implement an Enterprise Service Bus for service orchestration, mediation, routing anddecouple dependencies across systems."],"correct":[3],"explanation":"Implementing an Enterprise Service Bus (ESB) for service orchestration, mediation, routing and decoupling dependencies across systems is a better solution than refactoring the existing @future methods to use Enhanced External Services. An ESB can provide a centralized platform for integrating multiple systems and applications, while reducing the complexity and maintenance of point-to-point integrations. Enhanced External Services can be used to invoke external REST services from Salesforce, but they are not suitable for complex integration scenarios that require data transformation, error handling, or asynchronous processing. Reference: Salesforce Integration Architecture Designer Resource Guide, page 14"},{"id":108,"text":"What is the first thing an Integration Architect should validate if a callout from a LightningWeb Component to an external endpoint is failing?","options":["The endpoint domain has been added to Cross-Origin Resource Sharing.","The endpoint URL has been added to Content Security Policies.","The endpoint URL has added been to an outbound firewall rule.","The endpoint URL has been added to Remote Site Settings."],"correct":[3],"explanation":"The first thing an integration architect should validate if a callout from a Lightning Web Component to an external endpoint is failing is the endpoint domain has been added to Cross-Origin Resource Sharing (CORS). CORS is a mechanism that allows web browsers to make requests to servers on different origins, such as different domains, protocols, orports. CORS requires the server to send back a special header that indicates whether the browser is allowed to access the resource or not. If the endpoint domain is not added to the CORS whitelist in Salesforce, the browser will block the callout and throwan error. Option B is not correct because Content Security Policies (CSP) are used to control what resources can be loaded on a Visualforce or Lightning page, such as scripts, stylesheets, images, etc. CSP does not affect the callout from a Lightning WebComponent to an external endpoint. Option C is not correct because outbound firewall rules are used to restrict the network traffic from Salesforce to external systems. Firewall rules are configured at the network level, not at the Salesforce level. OptionD is not correct because Remote Site Settings are used to specify the domains that are allowed for callouts from Apex code, not from Lightning Web Components. References: Working with CORS and CSP to Call APIs from LWC [Cross-Origin Resource Sharing (CORS)]"},{"id":109,"text":"A company has an external system that processes and tracks orders. Sales reps manage their leads and opportunity pipeline in Salesforce. In the current state, the two systems are disconnected and processing orders requires a lot of manual entry on sales rep part. This creates delays in processing orders and incomplete data due to manual entry. As a part of modernization efforts, the company decided to integrate Salesforce and the order management system. The following technical requirements were identified: 1. Orders need to be created in real time from salesforce 2. Minimal customization*, and code should be written due to a tight timeline and lack of developer resources 3. Sales reps need to be able to see order history and be able to see most up to date information on current order status. 4. Managers need tobe able to run reports in Salesforce to see daily and monthly order volumes and fulfillment timelines. 5. The legacy system is hosted on premise and is currently connected to the Enterprise Service Bus (ESB). The ESB is flexible enough to provide any methods and connection types needed by salesforce team. 6. There are 1000 sales reps. Each user processes/creates on average 15 orders per shift. Most of the orders contain 20–30-line items. How should an integration architect integrate the two systems based on the technical requirements and system constraints?","options":["Use Salesforce external object and OData connector.","Use Salesforce custom object, custom REST API and ETL.","Use Salesforce standard object, REST API and ETL.","Use Salesforce big object, SOAP API and Dataloader."],"correct":[1],"explanation":"Using Salesforce custom object, custom REST API and ETL is a better solution than using Salesforce standard object, REST API and ETL. A custom object can store the order information and provide more flexibility and control overthe data model, validation rules, triggers, etc. A custom REST API can be used to create orders in real time from Salesforce, using the ESB as a proxy to communicate with the legacy system. An ETL tool can be used to sync the order history and status fromthe legacy system to Salesforce, using the last modified date as a filter. This way, sales reps can see the order history and the most up-to-date information on current order status, and managers can run reports on order volumes and fulfillment timelines. A standard object, such as Order, may not meet all the business requirements and may require more customization than a custom object. A standard REST API may not provide enough security and error handling for creating orders in real time. Reference: Salesforce Integration Architecture Designer Resource Guide, page 18 About Marks4sure.com marks4sure.com was founded in 2007. We provide latest & high quality IT / Business Certification Training Exam Questions, Study Guides, Practice Tests. We help you pass any IT / Business Certification Exams with 100% Pass Guaranteed or Full Refund. Especially Cisco, CompTIA, Citrix, EMC, HP, Oracle, VMware, Juniper, Check Point, LPI, Nortel, EXIN and so on. View list of all certification exams: All vendors We prepare state-of-the art practice tests for certification exams. You can reach us at any of the email addresses listed below. Sales: sales@marks4sure.com Feedback: feedback@marks4sure.com Support: support@marks4sure.com Any problems about IT certification or our products, You can write us back and we will get back to you within 24 hours."}];

        // Variabili globali
        let questions = [];
        let originalQuestionOrder = [];
        let currentQuestionIndex = 0;
        let userAnswers = [];
        let startTime = new Date();
        let randomQuestions = false;
        let randomAnswers = false;

        // Inizializzazione
        function initQuiz() {
            // Crea una copia delle domande originali
            questions = originalQuestions.map(q => ({
                ...q,
                options: [...q.options],
                correct: [...q.correct],
                originalCorrect: [...q.correct],
                originalOptions: [...q.options]
            }));
            
            // Salva l'ordine originale delle domande
            originalQuestionOrder = [...questions];
            
            // Applica randomizzazione se attiva
            if (randomQuestions) {
                shuffleArray(questions);
            }
            
            if (randomAnswers) {
                questions.forEach(question => randomizeAnswers(question));
            }
            
            currentQuestionIndex = 0;
            userAnswers = new Array(questions.length).fill(null).map(() => []);
            startTime = new Date();
            
            document.getElementById('totalQuestions').textContent = questions.length;
            document.getElementById('correctAnswers').textContent = '0';
            
            showQuestion();
            updateNavigation();
            updateProgress();
        }

        // Mostra la domanda corrente
        function showQuestion() {
            const question = questions[currentQuestionIndex];
            const isMultiple = question.correct.length > 1;
            
            document.getElementById('questionNumber').textContent = currentQuestionIndex + 1;
            document.getElementById('currentQuestion').textContent = currentQuestionIndex + 1;
            document.getElementById('questionText').textContent = question.text;
            document.getElementById('questionType').textContent = 
                isMultiple ? 'Risposta Multipla' : 'Risposta Singola';
            
            // Genera le opzioni
            const optionsContainer = document.getElementById('optionsContainer');
            optionsContainer.innerHTML = '';
            // Nascondi la spiegazione se visibile
            const explanationBox = document.getElementById('explanationBox');
            if (explanationBox) {
                explanationBox.classList.add('hidden');
                explanationBox.textContent = '';
            }

            
            question.options.forEach((option, index) => {
                const optionDiv = document.createElement('div');
                optionDiv.className = 'option';
                
                const inputType = isMultiple ? 'checkbox' : 'radio';
                const inputName = `question_${question.id}`;
                const isChecked = userAnswers[currentQuestionIndex].includes(index);
                
                optionDiv.innerHTML = `
                    <input type="${inputType}" 
                           id="option_${index}" 
                           name="${inputName}" 
                           value="${index}"
                           ${isChecked ? 'checked' : ''}
                           onchange="handleAnswerChange(${index}, this.checked)">
                    <label for="option_${index}" class="option-label">
                        <div class="checkmark"></div>
                        ${option}
                    </label>
                `;
                
                optionsContainer.appendChild(optionDiv);
            });
        }
        function goToQuestion() {
            const input = document.getElementById("gotoInput");
            const number = parseInt(input.value);

            if (!isNaN(number) && number >= 1 && number <= questions.length) {
                currentQuestionIndex = number - 1;
                showQuestion();
                updateNavigation();
                updateProgress();
                document.getElementById('quizContainer').classList.add('fade-in');
                setTimeout(() => {
                    document.getElementById('quizContainer').classList.remove('fade-in');
                }, 500);
            } else {
                alert("Inserisci un numero valido tra 1 e " + questions.length);
            }

            input.value = ''; // reset campo input
        }


        // Gestisce il cambiamento delle risposte
        function handleAnswerChange(optionIndex, isChecked) {
            const question = questions[currentQuestionIndex];
            const isMultiple = question.correct.length > 1;
            const optionsContainer = document.getElementById('optionsContainer');

            if (isMultiple) {
                if (isChecked) {
                    if (!userAnswers[currentQuestionIndex].includes(optionIndex)) {
                        userAnswers[currentQuestionIndex].push(optionIndex);
                    }
                } else {
                    userAnswers[currentQuestionIndex] = userAnswers[currentQuestionIndex]
                        .filter(answer => answer !== optionIndex);
                }
            } else {
                if (isChecked) {
                    userAnswers[currentQuestionIndex] = [optionIndex];
                }
            }

            // Rimuovi tutte le classi di evidenziazione precedenti
            optionsContainer.querySelectorAll('.option-label').forEach(label => {
                label.classList.remove('correct');
                label.classList.remove('incorrect');
            });

            // Verifica e colora
            userAnswers[currentQuestionIndex].forEach(selectedIdx => {
                const label = document.querySelector(`#option_${selectedIdx} + .option-label`);
                if (question.correct.includes(selectedIdx)) {
                    label.classList.add('correct');
                } else {
                    label.classList.add('incorrect');
                }
            });
        }

        // Naviga alla domanda successiva
        function nextQuestion() {
            if (currentQuestionIndex < questions.length - 1) {
                currentQuestionIndex++;
                showQuestion();
                updateNavigation();
                updateProgress();
                
                // Animazione
                document.getElementById('quizContainer').classList.add('fade-in');
                setTimeout(() => {
                    document.getElementById('quizContainer').classList.remove('fade-in');
                }, 500);
            } else {
                showResults();
            }
        }

        // Naviga alla domanda precedente
        function previousQuestion() {
            if (currentQuestionIndex > 0) {
                currentQuestionIndex--;
                showQuestion();
                updateNavigation();
                updateProgress();
                
                // Animazione
                document.getElementById('quizContainer').classList.add('fade-in');
                setTimeout(() => {
                    document.getElementById('quizContainer').classList.remove('fade-in');
                }, 500);
            }
        }

        // Aggiorna i pulsanti di navigazione
        function updateNavigation() {
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            
            prevBtn.disabled = currentQuestionIndex === 0;
            
            if (currentQuestionIndex === questions.length - 1) {
                nextBtn.innerHTML = '<i class="fas fa-check"></i> Termina Quiz';
            } else {
                nextBtn.innerHTML = 'Prossima <i class="fas fa-arrow-right"></i>';
            }
        }

        // Aggiorna la barra di progresso
        function updateProgress() {
            const progress = ((currentQuestionIndex + 1) / questions.length) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
        }

        // Calcola il punteggio
        function calculateScore() {
            let correct = 0;
            
            for (let i = 0; i < questions.length; i++) {
                const question = questions[i];
                const userAnswer = userAnswers[i].sort();
                const correctAnswer = question.correct.sort();
                
                // Confronta gli array di risposte
                if (JSON.stringify(userAnswer) === JSON.stringify(correctAnswer)) {
                    correct++;
                }
            }
            
            return {
                correct: correct,
                total: questions.length,
                percentage: Math.round((correct / questions.length) * 100)
            };
        }

        // Mostra i risultati
        function showResults() {
            const score = calculateScore();
            const endTime = new Date();
            const timeSpent = Math.round((endTime - startTime) / 1000);
            
            document.getElementById('quizContainer').classList.add('hidden');
            document.querySelector('.navigation').classList.add('hidden');
            document.querySelector('.controls').classList.add('hidden');
            
            const resultsContainer = document.getElementById('resultsContainer');
            resultsContainer.classList.remove('hidden');
            
            // Aggiorna il punteggio
            const scoreElement = document.getElementById('finalScore');
            scoreElement.textContent = score.percentage + '%';
            
            // Colora il punteggio in base al risultato
            if (score.percentage >= 80) {
                scoreElement.className = 'score';
            } else if (score.percentage >= 60) {
                scoreElement.className = 'score medium';
            } else {
                scoreElement.className = 'score low';
            }
            
            // Aggiorna le statistiche nell'header
            document.getElementById('correctAnswers').textContent = score.correct;
            
            // Genera i dettagli dei risultati
            const detailsContainer = document.getElementById('resultsDetails');
            detailsContainer.innerHTML = `
                <div class="result-card">
                    <div class="result-title">Punteggio Finale</div>
                    <div>${score.correct}/${score.total} (${score.percentage}%)</div>
                </div>
                <div class="result-card">
                    <div class="result-title">Tempo Impiegato</div>
                    <div>${Math.floor(timeSpent / 60)}m ${timeSpent % 60}s</div>
                </div>
                <div class="result-card">
                    <div class="result-title">Risposte Corrette</div>
                    <div>${score.correct}</div>
                </div>
                <div class="result-card ${score.total - score.correct > 0 ? 'incorrect' : ''}">
                    <div class="result-title">Risposte Sbagliate</div>
                    <div>${score.total - score.correct}</div>
                </div>
            `;
            
            // Effetto di celebrazione se il punteggio è alto
            if (score.percentage >= 80) {
                createCelebration();
            }
        }

        // Crea effetto di celebrazione
        function createCelebration() {
            const celebration = document.createElement('div');
            celebration.className = 'celebration';
            
            for (let i = 0; i < 50; i++) {
                const confetti = document.createElement('div');
                confetti.style.cssText = `
                    position: absolute;
                    width: 10px;
                    height: 10px;
                    background: ${['#4CAF50', '#2196F3', '#FF9800', '#E91E63'][Math.floor(Math.random() * 4)]};
                    left: ${Math.random() * 100}%;
                    top: -10px;
                    border-radius: 50%;
                    animation: confetti-fall ${2 + Math.random() * 3}s linear forwards;
                `;
                celebration.appendChild(confetti);
            }
            
            document.body.appendChild(celebration);
            
            // Rimuovi l'effetto dopo 5 secondi
            setTimeout(() => {
                celebration.remove();
            }, 5000);
            
            // CSS per l'animazione dei coriandoli
            if (!document.getElementById('confetti-style')) {
                const style = document.createElement('style');
                style.id = 'confetti-style';
                style.textContent = `
                    @keyframes confetti-fall {
                        to {
                            transform: translateY(100vh) rotate(720deg);
                            opacity: 0;
                        }
                    }
                `;
                document.head.appendChild(style);
            }
        }

        // Riavvia il quiz
        function restartQuiz() {
            document.getElementById('resultsContainer').classList.add('hidden');
            document.getElementById('quizContainer').classList.remove('hidden');
            document.querySelector('.navigation').classList.remove('hidden');
            document.querySelector('.controls').classList.remove('hidden');
            
            // Reset delle impostazioni di randomizzazione
            randomQuestions = false;
            randomAnswers = false;
            document.getElementById('randomQuestionsBtn').classList.remove('active');
            document.getElementById('randomQuestionsBtn').innerHTML = '<i class="fas fa-random"></i> Domande Random';
            document.getElementById('randomAnswersBtn').classList.remove('active');
            document.getElementById('randomAnswersBtn').innerHTML = '<i class="fas fa-shuffle"></i> Risposte Random';
            
            initQuiz();
        }

        // Inizializza il quiz al caricamento della pagina
        window.onload = function() {
            initQuiz();
        };

        // Funzioni per la randomizzazione
        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
            return array;
        }

        function randomizeAnswers(question) {
            // Crea un array di indici delle opzioni
            const indices = question.originalOptions.map((_, index) => index);
            
            // Mescola gli indici
            shuffleArray(indices);
            
            // Riordina le opzioni secondo gli indici mescolati
            question.options = indices.map(index => question.originalOptions[index]);
            
            // Aggiorna gli indici delle risposte corrette
            question.correct = question.originalCorrect.map(correctIndex => {
                return indices.indexOf(correctIndex);
            });
        }

        // Toggle per domande random
        function toggleRandomQuestions() {
            randomQuestions = !randomQuestions;
            const btn = document.getElementById('randomQuestionsBtn');
            
            if (randomQuestions) {
                btn.classList.add('active');
                btn.innerHTML = '<i class="fas fa-random"></i> Domande Random ON';
            } else {
                btn.classList.remove('active');
                btn.innerHTML = '<i class="fas fa-random"></i> Domande Random';
            }
            
            // Riavvia il quiz con le nuove impostazioni
            initQuiz();
        }

        // Toggle per risposte random
        function toggleRandomAnswers() {
            randomAnswers = !randomAnswers;
            const btn = document.getElementById('randomAnswersBtn');
            
            if (randomAnswers) {
                btn.classList.add('active');
                btn.innerHTML = '<i class="fas fa-shuffle"></i> Risposte Random ON';
            } else {
                btn.classList.remove('active');
                btn.innerHTML = '<i class="fas fa-shuffle"></i> Risposte Random';
            }
            
            // Riavvia il quiz con le nuove impostazioni
            initQuiz();
        }
        function showCorrectAnswer() {
            const question = questions[currentQuestionIndex];
            const optionsContainer = document.getElementById('optionsContainer');
            const explanationBox = document.getElementById('explanationBox');

            // Rimuove evidenziazioni precedenti
            optionsContainer.querySelectorAll('.option-label').forEach((label, idx) => {
                label.classList.remove('correct', 'incorrect');
                if (question.correct.includes(idx)) {
                    label.classList.add('correct');
                }
            });

            // Mostra explanation
            explanationBox.textContent = question.explanation;
            explanationBox.classList.remove('hidden');
        }

    </script>
</body>
</html>